{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "262f0c54",
   "metadata": {
    "_cell_guid": "fb1cf750-8783-46c7-8cec-cd968c6183b8",
    "_uuid": "3292b61f-161d-4d76-b300-be911957474e",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-05-13T11:42:19.475158Z",
     "iopub.status.busy": "2022-05-13T11:42:19.473319Z",
     "iopub.status.idle": "2022-05-13T11:42:41.265208Z",
     "shell.execute_reply": "2022-05-13T11:42:41.265796Z",
     "shell.execute_reply.started": "2022-05-06T03:33:27.70362Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 21.831367,
     "end_time": "2022-05-13T11:42:41.266123",
     "exception": false,
     "start_time": "2022-05-13T11:42:19.434756",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lxml in /opt/conda/lib/python3.7/site-packages (4.6.3)\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\r\n",
      "Collecting python-edgar\r\n",
      "  Downloading python_edgar-3.1.3-py3-none-any.whl (8.6 kB)\r\n",
      "Installing collected packages: python-edgar\r\n",
      "Successfully installed python-edgar-3.1.3\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\r\n",
      "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mode = ['full','cpu','gpu','wv'][0]\n",
    "\n",
    "# import libraries\n",
    "import os\n",
    "from os.path import isfile, isdir, join\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "from datetime import datetime, date\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from IPython.display import display\n",
    "from zipfile import ZipFile\n",
    "import pickle\n",
    "import unicodedata\n",
    "import pytz\n",
    "from joblib import Parallel, delayed\n",
    "import shutil\n",
    "import random\n",
    "import requests\n",
    "import gc\n",
    "import math\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import jaccard_score\n",
    "\n",
    "!pip install lxml\n",
    "import lxml\n",
    "\n",
    "!pip install python-edgar\n",
    "import edgar\n",
    "\n",
    "import nltk\n",
    "from nltk import tokenize\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89ff0058",
   "metadata": {
    "_cell_guid": "1cd97a69-a522-4b1d-af16-7481f9794ff6",
    "_uuid": "2968c996-f997-4ccb-a698-501cd88deeb4",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-05-13T11:42:41.349791Z",
     "iopub.status.busy": "2022-05-13T11:42:41.343179Z",
     "iopub.status.idle": "2022-05-13T11:42:41.353436Z",
     "shell.execute_reply": "2022-05-13T11:42:41.352792Z",
     "shell.execute_reply.started": "2022-05-06T03:33:48.964309Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.052548,
     "end_time": "2022-05-13T11:42:41.353590",
     "exception": false,
     "start_time": "2022-05-13T11:42:41.301042",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Handlers.SIG_DFL: 0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# log\n",
    "def log(msg):\n",
    "    now = datetime.strftime(datetime.now(tz=pytz.timezone('Hongkong')), '%Y-%m-%d %H:%M:%S')\n",
    "    print(f'[{now}] {msg}')\n",
    "    \n",
    "# pickle\n",
    "def save_pkl(obj, filename):\n",
    "    pickle.dump(obj, open(filename, 'wb'), protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    return\n",
    "def load_pkl(filename):\n",
    "    return pickle.load(open(filename, 'rb'))\n",
    "\n",
    "def get_size(path='.'):\n",
    "    total_size = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(path):\n",
    "        for f in filenames:\n",
    "            fp = os.path.join(dirpath, f)\n",
    "            # skip if it is symbolic link\n",
    "            if not os.path.islink(fp):\n",
    "                total_size += os.path.getsize(fp)\n",
    "    return total_size\n",
    "\n",
    "def dl_txt(txt):\n",
    "    with open(f'text.txt', 'w') as f:\n",
    "        f.write(txt)\n",
    "        f.close()\n",
    "    return\n",
    "\n",
    "import signal\n",
    "class TimeoutException(Exception):   # Custom exception class\n",
    "    pass\n",
    "def timeout_handler(signum, frame):   # Custom signal handler\n",
    "    raise TimeoutException\n",
    "signal.signal(signal.SIGALRM, timeout_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a590086",
   "metadata": {
    "_cell_guid": "b26df2de-e228-49ce-91dc-3340d2cfebab",
    "_uuid": "b88b55ab-23de-43a9-a4d3-c8cb1a7ec2f3",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-05-13T11:42:41.426403Z",
     "iopub.status.busy": "2022-05-13T11:42:41.425720Z",
     "iopub.status.idle": "2022-05-13T11:42:41.428200Z",
     "shell.execute_reply": "2022-05-13T11:42:41.427669Z",
     "shell.execute_reply.started": "2022-05-06T03:33:48.980597Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.040357,
     "end_time": "2022-05-13T11:42:41.428333",
     "exception": false,
     "start_time": "2022-05-13T11:42:41.387976",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# params\n",
    "params = dict()\n",
    "params['filing_start_date'] = '2008-01-01'\n",
    "params['filing_end_date'] = '2018-03-31'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ea9feb",
   "metadata": {
    "_cell_guid": "3efcb51c-f9c4-4402-8a44-dc34bae2d12d",
    "_uuid": "8c7743d4-f0f1-4a39-a035-2c13a8ebda23",
    "papermill": {
     "duration": 0.033175,
     "end_time": "2022-05-13T11:42:41.495151",
     "exception": false,
     "start_time": "2022-05-13T11:42:41.461976",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Create Master Index Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b1f8097",
   "metadata": {
    "_cell_guid": "0a2834de-635e-460c-ab9c-0303af843ea3",
    "_uuid": "40b093c6-354d-472f-a805-10889bcdf8d4",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-05-13T11:42:41.565525Z",
     "iopub.status.busy": "2022-05-13T11:42:41.564830Z",
     "iopub.status.idle": "2022-05-13T11:42:44.135021Z",
     "shell.execute_reply": "2022-05-13T11:42:44.135536Z",
     "shell.execute_reply.started": "2022-05-06T03:33:48.993084Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 2.607049,
     "end_time": "2022-05-13T11:42:44.135711",
     "exception": false,
     "start_time": "2022-05-13T11:42:41.528662",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cik</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stock</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [cik]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stock</th>\n",
       "      <th>cik</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>0000320193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>0000789019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GOOGL</td>\n",
       "      <td>0001652044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AMZN</td>\n",
       "      <td>0001018724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TSLA</td>\n",
       "      <td>0001318605</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   stock         cik\n",
       "0   AAPL  0000320193\n",
       "1   MSFT  0000789019\n",
       "2  GOOGL  0001652044\n",
       "3   AMZN  0001018724\n",
       "4   TSLA  0001318605"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "Get CIK mapping from 1) Wikipedia and 2) Edgar official mapping\n",
    "'''\n",
    "\n",
    "# current S&P500 CIK mapping based on wikipedia\n",
    "wiki_tbl_list = pd.read_html('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')\n",
    "curr_cons = wiki_tbl_list[0] \\\n",
    "            .assign(stock = lambda x: x.Symbol,\n",
    "                    cik = lambda x: x.CIK.astype(str).str.zfill(10)) \\\n",
    "            .loc[:,['stock','cik']]\n",
    "\n",
    "# Official Edgar Symbol-to-CIK mapping\n",
    "cik_map = pd.read_csv('https://www.sec.gov/include/ticker.txt', sep='\\t', names=['stock','cik']) \\\n",
    "                .assign(stock = lambda x: x.stock.str.upper(),\n",
    "                        cik = lambda x: x.cik.astype(str).str.zfill(10))\n",
    "\n",
    "# combine the two sources\n",
    "cik_map = pd.concat([cik_map, curr_cons.loc[lambda x: ~x.stock.isin(cik_map.stock)]], axis=0).drop_duplicates()\n",
    "display(cik_map.groupby('stock').count().loc[lambda x: x.cik>1])\n",
    "display(cik_map.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "251aaa33",
   "metadata": {
    "_cell_guid": "3687447d-12ee-4663-a5a3-2b23a2bf3e57",
    "_uuid": "dc4af405-913e-4716-8d2e-670d10a20577",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-05-13T11:42:44.208682Z",
     "iopub.status.busy": "2022-05-13T11:42:44.208030Z",
     "iopub.status.idle": "2022-05-13T11:42:45.127036Z",
     "shell.execute_reply": "2022-05-13T11:42:45.126353Z",
     "shell.execute_reply.started": "2022-05-06T03:33:51.751255Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.956325,
     "end_time": "2022-05-13T11:42:45.127188",
     "exception": false,
     "start_time": "2022-05-13T11:42:44.170863",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Perform CIK mapping on returns table\n",
    "'''\n",
    "\n",
    "# load full stock list based on returns table\n",
    "ret = pd.read_csv('../input/hkml-download-returns/ret.csv')\n",
    "ret = ret.set_index('date')\n",
    "ret.index = pd.to_datetime(ret.index)\n",
    "\n",
    "# derive EDGAR filing start (ret start date - 400 days) and end date\n",
    "df = []\n",
    "for stock in ret:\n",
    "    s = ret[stock].loc[lambda x: x.notnull()].index\n",
    "    df.append((stock, s.min(), s.max()))\n",
    "df = pd.DataFrame(df, columns=['stock','start_date','end_date'])\n",
    "df['start_date'] = df['start_date'] + np.timedelta64(-365*2,'D')\n",
    "\n",
    "# map to CIK\n",
    "stock_map = df.merge(cik_map, how='left', on='stock')\n",
    "assert stock_map.stock.nunique()==stock_map.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b938928d",
   "metadata": {
    "_cell_guid": "02ca3ff4-d278-40ec-b14e-2414f92c9a74",
    "_uuid": "bb53f7d2-cc79-45dc-8819-ac9acc74832f",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-05-13T11:42:45.207924Z",
     "iopub.status.busy": "2022-05-13T11:42:45.207079Z",
     "iopub.status.idle": "2022-05-13T11:42:45.246166Z",
     "shell.execute_reply": "2022-05-13T11:42:45.245605Z",
     "shell.execute_reply.started": "2022-05-06T03:33:52.678703Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.08317,
     "end_time": "2022-05-13T11:42:45.246319",
     "exception": false,
     "start_time": "2022-05-13T11:42:45.163149",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stock</th>\n",
       "      <th>start_date</th>\n",
       "      <th>end_date</th>\n",
       "      <th>cik</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ADS</td>\n",
       "      <td>2011-12-25</td>\n",
       "      <td>2018-03-26</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452</th>\n",
       "      <td>PBCT</td>\n",
       "      <td>2007-07-07</td>\n",
       "      <td>2018-03-26</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    stock start_date   end_date  cik\n",
       "14    ADS 2011-12-25 2018-03-26  NaN\n",
       "452  PBCT 2007-07-07 2018-03-26  NaN"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stock</th>\n",
       "      <th>start_date</th>\n",
       "      <th>end_date</th>\n",
       "      <th>cik</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>2007-07-07</td>\n",
       "      <td>2018-03-26</td>\n",
       "      <td>0001090872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AA</td>\n",
       "      <td>2007-07-07</td>\n",
       "      <td>2016-10-31</td>\n",
       "      <td>0001675149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAL</td>\n",
       "      <td>2013-03-24</td>\n",
       "      <td>2018-03-26</td>\n",
       "      <td>0000006201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAP</td>\n",
       "      <td>2013-07-09</td>\n",
       "      <td>2018-03-26</td>\n",
       "      <td>0001158449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2007-07-07</td>\n",
       "      <td>2018-03-26</td>\n",
       "      <td>0000320193</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  stock start_date   end_date         cik\n",
       "0     A 2007-07-07 2018-03-26  0001090872\n",
       "1    AA 2007-07-07 2016-10-31  0001675149\n",
       "2   AAL 2013-03-24 2018-03-26  0000006201\n",
       "3   AAP 2013-07-09 2018-03-26  0001158449\n",
       "4  AAPL 2007-07-07 2018-03-26  0000320193"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "Populate missing CIK\n",
    "'''\n",
    "\n",
    "# output missing CIK list\n",
    "stock_map[stock_map.cik.isnull()].to_csv('missing_stock_map.csv', index=False)\n",
    "\n",
    "# import manual mapping\n",
    "manual_cik_map = pd.read_csv('../input/nlp10k-manual-stock-cik-mapping/missing_stock_map.csv') \\\n",
    "    .assign(cik = lambda x: x.cik.astype(str).str.zfill(10)) \\\n",
    "    .rename(columns={'cik':'missing_cik'}) \\\n",
    "    .loc[:,['stock','missing_cik']]\n",
    "\n",
    "# fill in missing CIK\n",
    "stock_map = stock_map.merge(manual_cik_map, how='left', on='stock') \\\n",
    "    .assign(cik = lambda x: np.select([(x.cik.isnull()) & (x.missing_cik.notnull()), True],[x.missing_cik, x.cik])) \\\n",
    "    .drop('missing_cik', axis=1)\n",
    "\n",
    "# check if still missing any CIK\n",
    "display(stock_map.loc[lambda x: x.cik.isnull()])\n",
    "display(stock_map.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e87efbb9",
   "metadata": {
    "_cell_guid": "9ed6f2d6-acbb-4ba0-8081-0f256a921a6f",
    "_uuid": "690aa7dd-54f6-4f7d-a47a-750f32f16fa5",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-05-13T11:42:45.323590Z",
     "iopub.status.busy": "2022-05-13T11:42:45.322476Z",
     "iopub.status.idle": "2022-05-13T11:46:13.979165Z",
     "shell.execute_reply": "2022-05-13T11:46:13.975539Z",
     "shell.execute_reply.started": "2022-05-06T03:33:52.731507Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 208.69654,
     "end_time": "2022-05-13T11:46:13.979491",
     "exception": false,
     "start_time": "2022-05-13T11:42:45.282951",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-05-13 19:46:13] Shape of 10-K master_idx: (5272, 6)\n",
      "[2022-05-13 19:46:13] Shape of 10-Q master_idx: (15470, 6)\n",
      "[2022-05-13 19:46:13] Shape of 8-K master_idx: (74272, 6)\n",
      "[2022-05-13 19:46:13] Avg number of 10-K filing per stock: 8.742951907131012\n",
      "[2022-05-13 19:46:13] Avg number of 10-Q filing per stock: 25.612582781456954\n",
      "[2022-05-13 19:46:13] Avg number of 8-K filing per stock: 122.56105610561056\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filing_type</th>\n",
       "      <th>cik</th>\n",
       "      <th>filing_date</th>\n",
       "      <th>entity</th>\n",
       "      <th>full_submission_filename</th>\n",
       "      <th>index_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1436</th>\n",
       "      <td>10-K</td>\n",
       "      <td>0000091440</td>\n",
       "      <td>2008-02-19</td>\n",
       "      <td>SNAP ON INC</td>\n",
       "      <td>edgar/data/91440/0001104659-08-011401.txt</td>\n",
       "      <td>edgar/data/91440/0001104659-08-011401-index.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>862</th>\n",
       "      <td>10-K</td>\n",
       "      <td>0000050863</td>\n",
       "      <td>2008-02-20</td>\n",
       "      <td>INTEL CORP</td>\n",
       "      <td>edgar/data/50863/0000891618-08-000106.txt</td>\n",
       "      <td>edgar/data/50863/0000891618-08-000106-index.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3346</th>\n",
       "      <td>10-K</td>\n",
       "      <td>0000916076</td>\n",
       "      <td>2016-02-23</td>\n",
       "      <td>MARTIN MARIETTA MATERIALS INC</td>\n",
       "      <td>edgar/data/916076/0001193125-16-473754.txt</td>\n",
       "      <td>edgar/data/916076/0001193125-16-473754-index.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>10-K</td>\n",
       "      <td>0000008818</td>\n",
       "      <td>2010-03-01</td>\n",
       "      <td>AVERY DENNISON CORPORATION</td>\n",
       "      <td>edgar/data/8818/0000950123-10-018494.txt</td>\n",
       "      <td>edgar/data/8818/0000950123-10-018494-index.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>10-K</td>\n",
       "      <td>0000029534</td>\n",
       "      <td>2017-03-24</td>\n",
       "      <td>DOLLAR GENERAL CORP</td>\n",
       "      <td>edgar/data/29534/0001558370-17-002116.txt</td>\n",
       "      <td>edgar/data/29534/0001558370-17-002116-index.html</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     filing_type         cik filing_date                         entity  \\\n",
       "1436        10-K  0000091440  2008-02-19                    SNAP ON INC   \n",
       "862         10-K  0000050863  2008-02-20                     INTEL CORP   \n",
       "3346        10-K  0000916076  2016-02-23  MARTIN MARIETTA MATERIALS INC   \n",
       "169         10-K  0000008818  2010-03-01     AVERY DENNISON CORPORATION   \n",
       "473         10-K  0000029534  2017-03-24            DOLLAR GENERAL CORP   \n",
       "\n",
       "                        full_submission_filename  \\\n",
       "1436   edgar/data/91440/0001104659-08-011401.txt   \n",
       "862    edgar/data/50863/0000891618-08-000106.txt   \n",
       "3346  edgar/data/916076/0001193125-16-473754.txt   \n",
       "169     edgar/data/8818/0000950123-10-018494.txt   \n",
       "473    edgar/data/29534/0001558370-17-002116.txt   \n",
       "\n",
       "                                              index_url  \n",
       "1436   edgar/data/91440/0001104659-08-011401-index.html  \n",
       "862    edgar/data/50863/0000891618-08-000106-index.html  \n",
       "3346  edgar/data/916076/0001193125-16-473754-index.html  \n",
       "169     edgar/data/8818/0000950123-10-018494-index.html  \n",
       "473    edgar/data/29534/0001558370-17-002116-index.html  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "11    310\n",
       "10     48\n",
       "3      43\n",
       "4      38\n",
       "5      36\n",
       "6      32\n",
       "9      30\n",
       "7      29\n",
       "8      27\n",
       "2       8\n",
       "12      2\n",
       "Name: full_submission_filename, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 7s, sys: 19.7 s, total: 2min 26s\n",
      "Wall time: 3min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "'''\n",
    "Download master index from EDGAR\n",
    "'''\n",
    "# download all index\n",
    "edgar.download_index(dest='./', since_year=2006, user_agent='phyung0107@gmail.com', skip_all_present_except_last=False)\n",
    "\n",
    "# combin index\n",
    "master_idx = []\n",
    "for f in os.listdir('./'):\n",
    "    if '.tsv' in f:\n",
    "        df = pd.read_csv(f'./{f}', sep='|', names=['cik','entity','filing_type','filing_date','full_submission_filename','index_url'])\n",
    "        master_idx.append(df)\n",
    "        os.remove(f'./{f}')\n",
    "master_idx = pd.concat(master_idx)\n",
    "\n",
    "# cleaning and filter with only filings required\n",
    "master_idx = master_idx \\\n",
    "    .assign(cik = lambda x: x.cik.astype(str).str.zfill(10),\n",
    "            filing_date = lambda x: pd.to_datetime(x.filing_date)) \\\n",
    "    .merge(stock_map, how='inner', on='cik') \\\n",
    "    .loc[lambda x: (x.filing_date >= x.start_date) & (x.filing_date <= x.end_date)] \\\n",
    "    .reset_index(drop=True)\n",
    "\n",
    "# if duplicate, take last entry\n",
    "master_idx = master_idx \\\n",
    "    .sort_values(['filing_type','cik','filing_date','full_submission_filename']) \\\n",
    "    .groupby(['filing_type','cik','filing_date']) \\\n",
    "    .last() \\\n",
    "    .reset_index()\n",
    "\n",
    "# remove stocks with only 1 10-k or 10-Q filing\n",
    "ciks_10k = master_idx.loc[lambda x: x.filing_type=='10-K'].groupby('cik')['full_submission_filename'].nunique().loc[lambda x: x==1].index\n",
    "ciks_10q = master_idx.loc[lambda x: x.filing_type=='10-Q'].groupby('cik')['full_submission_filename'].nunique().loc[lambda x: x==1].index\n",
    "master_idx = master_idx.loc[lambda x: (~x.cik.isin(ciks_10k)) & (~x.cik.isin(ciks_10q))]\n",
    "\n",
    "# save the CIK-stock mapping\n",
    "cik_map = master_idx[['cik','stock']].drop_duplicates()\n",
    "\n",
    "# final clean\n",
    "master_idx = master_idx \\\n",
    "    .drop(['stock','start_date','end_date'], axis=1) \\\n",
    "    .drop_duplicates() \\\n",
    "    .sort_values(['filing_type','cik','filing_date']) \\\n",
    "    .reset_index(drop=True)\n",
    "\n",
    "# separate 10-K, 10-Q, 8-K\n",
    "master_idx_10q = master_idx.loc[lambda x: x.filing_type=='10-Q'].reset_index(drop=True)\n",
    "master_idx_8k = master_idx.loc[lambda x: x.filing_type=='8-K'].reset_index(drop=True)\n",
    "master_idx = master_idx.loc[lambda x: x.filing_type=='10-K'].reset_index(drop=True)\n",
    "\n",
    "log(f'Shape of 10-K master_idx: {master_idx.shape}')\n",
    "log(f'Shape of 10-Q master_idx: {master_idx_10q.shape}')\n",
    "log(f'Shape of 8-K master_idx: {master_idx_8k.shape}')\n",
    "log(f'Avg number of 10-K filing per stock: {master_idx.shape[0] / master_idx.cik.nunique()}')\n",
    "log(f'Avg number of 10-Q filing per stock: {master_idx_10q.shape[0] / master_idx_10q.cik.nunique()}')\n",
    "log(f'Avg number of 8-K filing per stock: {master_idx_8k.shape[0] / master_idx_8k.cik.nunique()}')\n",
    "display(master_idx.sample(5))\n",
    "display(master_idx.groupby('cik')['full_submission_filename'].nunique().value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3037f14b",
   "metadata": {
    "_cell_guid": "d83f2e95-4b8c-49eb-a1f1-b49c1beeb7ec",
    "_uuid": "9af53acb-1eb6-435e-923b-6a0f9230b3c7",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-05-13T11:46:14.061143Z",
     "iopub.status.busy": "2022-05-13T11:46:14.060391Z",
     "iopub.status.idle": "2022-05-13T11:46:14.063851Z",
     "shell.execute_reply": "2022-05-13T11:46:14.063287Z",
     "shell.execute_reply.started": "2022-05-06T03:37:21.968764Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.046352,
     "end_time": "2022-05-13T11:46:14.064000",
     "exception": false,
     "start_time": "2022-05-13T11:46:14.017648",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # for testing\n",
    "# master_idx = master_idx.loc[lambda x: x.cik.isin(master_idx.cik.sample(3).tolist())].reset_index(drop=True)\n",
    "# master_idx = master_idx.loc[lambda x: x.cik=='0000020286'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3605e1e",
   "metadata": {
    "_cell_guid": "27b73346-1fca-4794-8b18-fbd1b1d44c97",
    "_uuid": "3eb23e6c-1113-4d95-b2c0-776c784ea01f",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-05-13T11:46:14.964551Z",
     "iopub.status.busy": "2022-05-13T11:46:14.963780Z",
     "iopub.status.idle": "2022-05-13T11:55:51.880490Z",
     "shell.execute_reply": "2022-05-13T11:55:51.879042Z",
     "shell.execute_reply.started": "2022-05-06T03:37:21.97516Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 577.778391,
     "end_time": "2022-05-13T11:55:51.880640",
     "exception": false,
     "start_time": "2022-05-13T11:46:14.102249",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-05-13 19:55:51] Percentage of null: 0.0\n",
      "[2022-05-13 19:55:51] Percentage of PDF: 0.011760242792109257\n",
      "[2022-05-13 19:55:51] Number of CIK with single doc: 1\n",
      "[2022-05-13 19:55:51] Shape of master_idx: (5186, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filing_type</th>\n",
       "      <th>cik</th>\n",
       "      <th>filing_date</th>\n",
       "      <th>entity</th>\n",
       "      <th>full_submission_filename</th>\n",
       "      <th>index_url</th>\n",
       "      <th>url_10k</th>\n",
       "      <th>doc_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3193</th>\n",
       "      <td>10-K</td>\n",
       "      <td>0000909954</td>\n",
       "      <td>2012-11-28</td>\n",
       "      <td>GREEN MOUNTAIN COFFEE ROASTERS INC</td>\n",
       "      <td>edgar/data/909954/0001104659-12-080228.txt</td>\n",
       "      <td>edgar/data/909954/0001104659-12-080228-index.html</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/909954...</td>\n",
       "      <td>0000909954_20121128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>10-K</td>\n",
       "      <td>0000009389</td>\n",
       "      <td>2011-02-28</td>\n",
       "      <td>BALL CORP</td>\n",
       "      <td>edgar/data/9389/0000009389-11-000014.txt</td>\n",
       "      <td>edgar/data/9389/0000009389-11-000014-index.html</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/9389/0...</td>\n",
       "      <td>0000009389_20110228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1509</th>\n",
       "      <td>10-K</td>\n",
       "      <td>0000096021</td>\n",
       "      <td>2016-08-30</td>\n",
       "      <td>SYSCO CORP</td>\n",
       "      <td>edgar/data/96021/0000096021-16-000275.txt</td>\n",
       "      <td>edgar/data/96021/0000096021-16-000275-index.html</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/96021/...</td>\n",
       "      <td>0000096021_20160830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>10-K</td>\n",
       "      <td>0000020520</td>\n",
       "      <td>2016-02-25</td>\n",
       "      <td>FRONTIER COMMUNICATIONS CORP</td>\n",
       "      <td>edgar/data/20520/0000020520-16-000076.txt</td>\n",
       "      <td>edgar/data/20520/0000020520-16-000076-index.html</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/20520/...</td>\n",
       "      <td>0000020520_20160225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3148</th>\n",
       "      <td>10-K</td>\n",
       "      <td>0000899689</td>\n",
       "      <td>2013-02-26</td>\n",
       "      <td>VORNADO REALTY TRUST</td>\n",
       "      <td>edgar/data/899689/0000899689-13-000004.txt</td>\n",
       "      <td>edgar/data/899689/0000899689-13-000004-index.html</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/899689...</td>\n",
       "      <td>0000899689_20130226</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     filing_type         cik filing_date                              entity  \\\n",
       "3193        10-K  0000909954  2012-11-28  GREEN MOUNTAIN COFFEE ROASTERS INC   \n",
       "188         10-K  0000009389  2011-02-28                           BALL CORP   \n",
       "1509        10-K  0000096021  2016-08-30                          SYSCO CORP   \n",
       "339         10-K  0000020520  2016-02-25        FRONTIER COMMUNICATIONS CORP   \n",
       "3148        10-K  0000899689  2013-02-26                VORNADO REALTY TRUST   \n",
       "\n",
       "                        full_submission_filename  \\\n",
       "3193  edgar/data/909954/0001104659-12-080228.txt   \n",
       "188     edgar/data/9389/0000009389-11-000014.txt   \n",
       "1509   edgar/data/96021/0000096021-16-000275.txt   \n",
       "339    edgar/data/20520/0000020520-16-000076.txt   \n",
       "3148  edgar/data/899689/0000899689-13-000004.txt   \n",
       "\n",
       "                                              index_url  \\\n",
       "3193  edgar/data/909954/0001104659-12-080228-index.html   \n",
       "188     edgar/data/9389/0000009389-11-000014-index.html   \n",
       "1509   edgar/data/96021/0000096021-16-000275-index.html   \n",
       "339    edgar/data/20520/0000020520-16-000076-index.html   \n",
       "3148  edgar/data/899689/0000899689-13-000004-index.html   \n",
       "\n",
       "                                                url_10k               doc_id  \n",
       "3193  https://www.sec.gov/Archives/edgar/data/909954...  0000909954_20121128  \n",
       "188   https://www.sec.gov/Archives/edgar/data/9389/0...  0000009389_20110228  \n",
       "1509  https://www.sec.gov/Archives/edgar/data/96021/...  0000096021_20160830  \n",
       "339   https://www.sec.gov/Archives/edgar/data/20520/...  0000020520_20160225  \n",
       "3148  https://www.sec.gov/Archives/edgar/data/899689...  0000899689_20130226  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 39.3 s, sys: 1.52 s, total: 40.9 s\n",
      "Wall time: 9min 36s\n",
      "Parser   : 818 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "'''\n",
    "contruct full 10-K HTML URLs\n",
    "'''\n",
    "def get_html_link(i, full_submission_filename, index_url):\n",
    "    time.sleep(0.1)\n",
    "    try: \n",
    "        # get 10-K document name\n",
    "        url = f'https://www.sec.gov/Archives/{index_url}'\n",
    "        html = requests.get(url, headers={\"user-agent\": f\"chan_tai_man_{int(float(np.random.rand(1)) * 1e7)}@gmail.com\"}).content\n",
    "        doc_name = pd.read_html(html)[0] \\\n",
    "            .loc[lambda x: x.Type=='10-K'] \\\n",
    "            .sort_values('Size', ascending=False) \\\n",
    "            .Document \\\n",
    "            .tolist()[0]\n",
    "\n",
    "        # construct full URL\n",
    "        filing_id = full_submission_filename.replace('.txt','').replace('-','')\n",
    "        full_url = f'https://www.sec.gov/Archives/{filing_id}/{doc_name}'\n",
    "    except:\n",
    "        full_url = None\n",
    "    \n",
    "    log(f'[{i}] {full_url}') if i%200==0 else None\n",
    "    return i, full_url\n",
    "\n",
    "results = Parallel(n_jobs=-1)(delayed(get_html_link)(i, master_idx.iloc[i]['full_submission_filename'], master_idx.iloc[i]['index_url']) for i in range(len(master_idx)))\n",
    "results = pd.DataFrame(results, columns=['i','url_10k']).set_index('i')\n",
    "master_idx = master_idx.merge(results, how='left', left_index=True, right_index=True)\n",
    "\n",
    "# remove nulls and pdf\n",
    "log(f'Percentage of null: {master_idx[\"url_10k\"].isnull().sum() / master_idx.shape[0]}')\n",
    "log(f'Percentage of PDF: {(master_idx[\"url_10k\"].fillna(\"\").str.lower().str[-3:]==\"pdf\").sum() / master_idx.shape[0]}')\n",
    "master_idx = master_idx.loc[lambda x: (x.url_10k.fillna('').str.lower().str[-3:].isin(['htm','tml']))].reset_index(drop=True)\n",
    "\n",
    "# check again CIK with single doc\n",
    "ciks = master_idx.groupby('cik')['filing_date'].count().loc[lambda x: x<2].index.tolist()\n",
    "log(f'Number of CIK with single doc: {len(ciks)}')\n",
    "master_idx = master_idx.loc[lambda x: ~x.cik.isin(ciks)].reset_index(drop=True)\n",
    "\n",
    "# assign doc_id\n",
    "master_idx = master_idx \\\n",
    "    .assign(doc_id = lambda x: x.cik + '_' + x.filing_date.apply(lambda y: str(y)[:10].replace('-',''))) \\\n",
    "    .sort_values('doc_id') \\\n",
    "    .reset_index(drop=True)\n",
    "\n",
    "assert master_idx.doc_id.nunique()==master_idx.shape[0]\n",
    "log(f'Shape of master_idx: {master_idx.shape}')\n",
    "display(master_idx.sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d57ed92",
   "metadata": {
    "_cell_guid": "27b73346-1fca-4794-8b18-fbd1b1d44c97",
    "_uuid": "3eb23e6c-1113-4d95-b2c0-776c784ea01f",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-05-13T11:55:51.974124Z",
     "iopub.status.busy": "2022-05-13T11:55:51.971114Z",
     "iopub.status.idle": "2022-05-13T12:23:55.298280Z",
     "shell.execute_reply": "2022-05-13T12:23:55.298796Z",
     "shell.execute_reply.started": "2022-05-06T03:47:19.65819Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 1683.378936,
     "end_time": "2022-05-13T12:23:55.298999",
     "exception": false,
     "start_time": "2022-05-13T11:55:51.920063",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-05-13 20:23:54] Percentage of null: 0.0\n",
      "[2022-05-13 20:23:54] Percentage of PDF: 0.009372979961215255\n",
      "[2022-05-13 20:23:54] Number of CIK with single doc: 0\n",
      "[2022-05-13 20:23:55] Shape of master_idx_10q: (15238, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filing_type</th>\n",
       "      <th>cik</th>\n",
       "      <th>filing_date</th>\n",
       "      <th>entity</th>\n",
       "      <th>full_submission_filename</th>\n",
       "      <th>index_url</th>\n",
       "      <th>url_10q</th>\n",
       "      <th>doc_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9685</th>\n",
       "      <td>10-Q</td>\n",
       "      <td>0000916365</td>\n",
       "      <td>2014-08-04</td>\n",
       "      <td>TRACTOR SUPPLY CO /DE/</td>\n",
       "      <td>edgar/data/916365/0000916365-14-000121.txt</td>\n",
       "      <td>edgar/data/916365/0000916365-14-000121-index.html</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/916365...</td>\n",
       "      <td>0000916365_20140804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10104</th>\n",
       "      <td>10-Q</td>\n",
       "      <td>0000936468</td>\n",
       "      <td>2017-07-20</td>\n",
       "      <td>LOCKHEED MARTIN CORP</td>\n",
       "      <td>edgar/data/936468/0001193125-17-232077.txt</td>\n",
       "      <td>edgar/data/936468/0001193125-17-232077-index.html</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/936468...</td>\n",
       "      <td>0000936468_20170720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10763</th>\n",
       "      <td>10-Q</td>\n",
       "      <td>0001018963</td>\n",
       "      <td>2012-05-04</td>\n",
       "      <td>ALLEGHENY TECHNOLOGIES INC</td>\n",
       "      <td>edgar/data/1018963/0001193125-12-212043.txt</td>\n",
       "      <td>edgar/data/1018963/0001193125-12-212043-index....</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/101896...</td>\n",
       "      <td>0001018963_20120504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14705</th>\n",
       "      <td>10-Q</td>\n",
       "      <td>0001466258</td>\n",
       "      <td>2014-07-22</td>\n",
       "      <td>Ingersoll-Rand plc</td>\n",
       "      <td>edgar/data/1466258/0001466258-14-000044.txt</td>\n",
       "      <td>edgar/data/1466258/0001466258-14-000044-index....</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/146625...</td>\n",
       "      <td>0001466258_20140722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11220</th>\n",
       "      <td>10-Q</td>\n",
       "      <td>0001039684</td>\n",
       "      <td>2009-11-05</td>\n",
       "      <td>ONEOK INC /NEW/</td>\n",
       "      <td>edgar/data/1039684/0001039684-09-000083.txt</td>\n",
       "      <td>edgar/data/1039684/0001039684-09-000083-index....</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/103968...</td>\n",
       "      <td>0001039684_20091105</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      filing_type         cik filing_date                      entity  \\\n",
       "9685         10-Q  0000916365  2014-08-04      TRACTOR SUPPLY CO /DE/   \n",
       "10104        10-Q  0000936468  2017-07-20        LOCKHEED MARTIN CORP   \n",
       "10763        10-Q  0001018963  2012-05-04  ALLEGHENY TECHNOLOGIES INC   \n",
       "14705        10-Q  0001466258  2014-07-22          Ingersoll-Rand plc   \n",
       "11220        10-Q  0001039684  2009-11-05             ONEOK INC /NEW/   \n",
       "\n",
       "                          full_submission_filename  \\\n",
       "9685    edgar/data/916365/0000916365-14-000121.txt   \n",
       "10104   edgar/data/936468/0001193125-17-232077.txt   \n",
       "10763  edgar/data/1018963/0001193125-12-212043.txt   \n",
       "14705  edgar/data/1466258/0001466258-14-000044.txt   \n",
       "11220  edgar/data/1039684/0001039684-09-000083.txt   \n",
       "\n",
       "                                               index_url  \\\n",
       "9685   edgar/data/916365/0000916365-14-000121-index.html   \n",
       "10104  edgar/data/936468/0001193125-17-232077-index.html   \n",
       "10763  edgar/data/1018963/0001193125-12-212043-index....   \n",
       "14705  edgar/data/1466258/0001466258-14-000044-index....   \n",
       "11220  edgar/data/1039684/0001039684-09-000083-index....   \n",
       "\n",
       "                                                 url_10q               doc_id  \n",
       "9685   https://www.sec.gov/Archives/edgar/data/916365...  0000916365_20140804  \n",
       "10104  https://www.sec.gov/Archives/edgar/data/936468...  0000936468_20170720  \n",
       "10763  https://www.sec.gov/Archives/edgar/data/101896...  0001018963_20120504  \n",
       "14705  https://www.sec.gov/Archives/edgar/data/146625...  0001466258_20140722  \n",
       "11220  https://www.sec.gov/Archives/edgar/data/103968...  0001039684_20091105  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 55s, sys: 3.58 s, total: 1min 58s\n",
      "Wall time: 28min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "'''\n",
    "contruct full 10-Q HTML URLs\n",
    "'''\n",
    "def get_html_link(i, full_submission_filename, index_url):\n",
    "    time.sleep(0.1)\n",
    "    try: \n",
    "        # get 10-K document name\n",
    "        url = f'https://www.sec.gov/Archives/{index_url}'\n",
    "        html = requests.get(url, headers={\"user-agent\": f\"chan_tai_man_{int(float(np.random.rand(1)) * 1e7)}@gmail.com\"}).content\n",
    "        doc_name = pd.read_html(html)[0] \\\n",
    "            .loc[lambda x: x.Type=='10-Q'] \\\n",
    "            .sort_values('Size', ascending=False) \\\n",
    "            .Document \\\n",
    "            .tolist()[0]\n",
    "\n",
    "        # construct full URL\n",
    "        filing_id = full_submission_filename.replace('.txt','').replace('-','')\n",
    "        full_url = f'https://www.sec.gov/Archives/{filing_id}/{doc_name}'\n",
    "    except:\n",
    "        full_url = None\n",
    "    \n",
    "    log(f'[{i}] {full_url}') if i%200==0 else None\n",
    "    return i, full_url\n",
    "\n",
    "results = Parallel(n_jobs=-1)(delayed(get_html_link)(i, master_idx_10q.iloc[i]['full_submission_filename'], master_idx_10q.iloc[i]['index_url']) for i in range(len(master_idx_10q)))\n",
    "results = pd.DataFrame(results, columns=['i','url_10q']).set_index('i')\n",
    "master_idx_10q = master_idx_10q.merge(results, how='left', left_index=True, right_index=True)\n",
    "\n",
    "# remove nulls and pdf\n",
    "log(f'Percentage of null: {master_idx_10q[\"url_10q\"].isnull().sum() / master_idx_10q.shape[0]}')\n",
    "log(f'Percentage of PDF: {(master_idx_10q[\"url_10q\"].fillna(\"\").str.lower().str[-3:]==\"pdf\").sum() / master_idx_10q.shape[0]}')\n",
    "master_idx_10q = master_idx_10q.loc[lambda x: (x.url_10q.fillna('').str.lower().str[-3:].isin(['htm','tml']))].reset_index(drop=True)\n",
    "\n",
    "# check again CIK with single doc\n",
    "ciks = master_idx_10q.groupby('cik')['filing_date'].count().loc[lambda x: x<2].index.tolist()\n",
    "log(f'Number of CIK with single doc: {len(ciks)}')\n",
    "master_idx_10q = master_idx_10q.loc[lambda x: ~x.cik.isin(ciks)].reset_index(drop=True)\n",
    "\n",
    "# assign doc_id\n",
    "master_idx_10q = master_idx_10q \\\n",
    "    .assign(doc_id = lambda x: x.cik + '_' + x.filing_date.apply(lambda y: str(y)[:10].replace('-',''))) \\\n",
    "    .sort_values('doc_id') \\\n",
    "    .reset_index(drop=True)\n",
    "\n",
    "assert master_idx_10q.doc_id.nunique()==master_idx_10q.shape[0]\n",
    "log(f'Shape of master_idx_10q: {master_idx_10q.shape}')\n",
    "display(master_idx_10q.sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08244d95",
   "metadata": {
    "_cell_guid": "3b23ba52-4836-4b79-910c-56665465fefc",
    "_uuid": "0d545557-b67a-4278-b90b-af265f9614b1",
    "papermill": {
     "duration": 0.040997,
     "end_time": "2022-05-13T12:23:55.383792",
     "exception": false,
     "start_time": "2022-05-13T12:23:55.342795",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Document cleaning and Item extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86841017",
   "metadata": {
    "_cell_guid": "344862d0-006b-45cb-9443-d5e48121e375",
    "_uuid": "3cda8346-5bcc-4cd3-9b3a-5ec1a6718acd",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-05-13T12:23:55.482482Z",
     "iopub.status.busy": "2022-05-13T12:23:55.476597Z",
     "iopub.status.idle": "2022-05-13T12:23:55.486067Z",
     "shell.execute_reply": "2022-05-13T12:23:55.485508Z",
     "shell.execute_reply.started": "2022-05-06T04:16:00.19709Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.059978,
     "end_time": "2022-05-13T12:23:55.486228",
     "exception": false,
     "start_time": "2022-05-13T12:23:55.426250",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_unicode1(txt):\n",
    "    chars = {\n",
    "        r'[\\xc2\\x82]' : ',',        # High code comma\n",
    "         r'[\\xc2\\x84]' : ',,',       # High code double comma\n",
    "         r'[\\xc2\\x85]' : '...',      # Tripple dot\n",
    "         r'[\\xc2\\x88]' : '^',        # High carat\n",
    "         r'[\\xc2\\x91]' : \"'\",     # Forward single quote\n",
    "         r'[\\xc2\\x92]' : \"'\",     # Reverse single quote\n",
    "         r'[\\xc2\\x93]' : '\"',     # Forward double quote\n",
    "         r'[\\xc2\\x94]' : '\"',     # Reverse double quote\n",
    "         r'[\\xc2\\x95]' : ' ',\n",
    "         r'[\\xc2\\x96]' : '-',        # High hyphen\n",
    "         r'[\\xc2\\x97]' : '--',       # Double hyphen\n",
    "         r'[\\xc2\\x99]' : ' ',\n",
    "         r'[\\xc2\\xa0]' : ' ',\n",
    "         r'[\\xc2\\xa6]' : '|',        # Split vertical bar\n",
    "         r'[\\xc2\\xab]' : '<<',       # Double less than\n",
    "         r'[\\xc2\\xbb]' : '>>',       # Double greater than\n",
    "         r'[\\xc2\\xbc]' : '1/4',      # one quarter\n",
    "         r'[\\xc2\\xbd]' : '1/2',      # one half\n",
    "         r'[\\xc2\\xbe]' : '3/4',      # three quarters\n",
    "         r'[\\xca\\xbf]' : \"'\",     # c-single quote\n",
    "         r'[\\xcc\\xa8]' : '',         # modifier - under curve\n",
    "         r'[\\xcc\\xb1]' : '',          # modifier - under line\n",
    "         r\"[\\']\" : \"'\"\n",
    "    }\n",
    "    for ptrn in chars:\n",
    "        txt = re.sub(ptrn, chars[ptrn], txt)\n",
    "    return txt\n",
    "\n",
    "def remove_unicode2(txt):\n",
    "    txt = txt. \\\n",
    "        replace('\\\\xe2\\\\x80\\\\x99', \"'\"). \\\n",
    "        replace('\\\\xc3\\\\xa9', 'e'). \\\n",
    "        replace('\\\\xe2\\\\x80\\\\x90', '-'). \\\n",
    "        replace('\\\\xe2\\\\x80\\\\x91', '-'). \\\n",
    "        replace('\\\\xe2\\\\x80\\\\x92', '-'). \\\n",
    "        replace('\\\\xe2\\\\x80\\\\x93', '-'). \\\n",
    "        replace('\\\\xe2\\\\x80\\\\x94', '-'). \\\n",
    "        replace('\\\\xe2\\\\x80\\\\x94', '-'). \\\n",
    "        replace('\\\\xe2\\\\x80\\\\x98', \"'\"). \\\n",
    "        replace('\\\\xe2\\\\x80\\\\x9b', \"'\"). \\\n",
    "        replace('\\\\xe2\\\\x80\\\\x9c', '\"'). \\\n",
    "        replace('\\\\xe2\\\\x80\\\\x9c', '\"'). \\\n",
    "        replace('\\\\xe2\\\\x80\\\\x9d', '\"'). \\\n",
    "        replace('\\\\xe2\\\\x80\\\\x9e', '\"'). \\\n",
    "        replace('\\\\xe2\\\\x80\\\\x9f', '\"'). \\\n",
    "        replace('\\\\xe2\\\\x80\\\\xa6', '...'). \\\n",
    "        replace('\\\\xe2\\\\x80\\\\xb2', \"'\"). \\\n",
    "        replace('\\\\xe2\\\\x80\\\\xb3', \"'\"). \\\n",
    "        replace('\\\\xe2\\\\x80\\\\xb4', \"'\"). \\\n",
    "        replace('\\\\xe2\\\\x80\\\\xb5', \"'\"). \\\n",
    "        replace('\\\\xe2\\\\x80\\\\xb6', \"'\"). \\\n",
    "        replace('\\\\xe2\\\\x80\\\\xb7', \"'\"). \\\n",
    "        replace('\\\\xe2\\\\x81\\\\xba', \"+\"). \\\n",
    "        replace('\\\\xe2\\\\x81\\\\xbb', \"-\"). \\\n",
    "        replace('\\\\xe2\\\\x81\\\\xbc', \"=\"). \\\n",
    "        replace('\\\\xe2\\\\x81\\\\xbd', \"(\"). \\\n",
    "        replace('\\\\xe2\\\\x81\\\\xbe', \")\")\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26f7d7c0",
   "metadata": {
    "_cell_guid": "85890bee-76f6-4717-8355-d18e43e45403",
    "_uuid": "cca5fb03-a757-429e-aacf-a42c97a66ba1",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-05-13T12:23:55.576599Z",
     "iopub.status.busy": "2022-05-13T12:23:55.575655Z",
     "iopub.status.idle": "2022-05-13T12:23:55.578533Z",
     "shell.execute_reply": "2022-05-13T12:23:55.578005Z",
     "shell.execute_reply.started": "2022-05-06T04:16:00.212298Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.051537,
     "end_time": "2022-05-13T12:23:55.578679",
     "exception": false,
     "start_time": "2022-05-13T12:23:55.527142",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_doc1(txt):\n",
    "\n",
    "    # remove all special fields e.g. us-gaap:AccumulatedOtherComprehensiveIncomeMember\n",
    "    txt = re.sub(r'\\b' + re.escape('us-gaap:') + r'\\w+\\b', '', txt)\n",
    "    txt = re.sub(r'\\b\\w+[:]\\w+\\b', '', txt)\n",
    "\n",
    "    # remove unicode characters\n",
    "    txt = unicodedata.normalize(\"NFKD\", txt)\n",
    "    txt = remove_unicode1(txt)\n",
    "    txt = remove_unicode2(txt)\n",
    "\n",
    "    # standardize spaces\n",
    "    txt = txt.replace('\\\\n',' ').replace('\\n',' ').replace('\\\\t','|').replace('\\t','|')\n",
    "    txt = re.sub(r'\\| +', '|', txt)\n",
    "    txt = re.sub(r' +\\|', '|', txt)\n",
    "    txt = re.sub(r'\\|+', '|', txt)\n",
    "    txt = re.sub(r' +', ' ', txt)\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "692853bd",
   "metadata": {
    "_cell_guid": "4e4e4d68-b882-428e-b83c-7eed82544498",
    "_uuid": "61c548a9-18a4-4e66-80af-9d2a8e629d3d",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-05-13T12:23:55.670796Z",
     "iopub.status.busy": "2022-05-13T12:23:55.669811Z",
     "iopub.status.idle": "2022-05-13T12:23:55.672272Z",
     "shell.execute_reply": "2022-05-13T12:23:55.672890Z",
     "shell.execute_reply.started": "2022-05-06T04:16:00.233323Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.053726,
     "end_time": "2022-05-13T12:23:55.673060",
     "exception": false,
     "start_time": "2022-05-13T12:23:55.619334",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Function to clean txt; applied only after Item extraction\n",
    "'''\n",
    "def clean_doc2(txt):\n",
    "    # lowercase all strings\n",
    "    txt = txt.lower()\n",
    "    # replace sep with space\n",
    "    txt = txt.replace('|',' ')\n",
    "    # remove tags\n",
    "    txt = re.sub('<.+>', '', txt)\n",
    "    # remove unwanted characters, numbers, dots\n",
    "    txt = re.sub(r'([a-z]+\\d+)+([a-z]+)?(\\.+)?', '', txt) # aa12bb33. y3y\n",
    "    txt = re.sub(r'(\\d+[a-z]+)+(\\d+)?(\\.+)?', '', txt) # 1a2b. 1a1a1\n",
    "    txt = re.sub(r'\\b\\$?\\d+\\.(\\d+)?', '', txt) # $2.14 999.8 123.\n",
    "    txt = re.sub(r'\\$\\d+', '', txt) # $88\n",
    "    txt = re.sub(r'(\\w+\\.){2,}(\\w+)?', '', txt) # W.C. ASD.ASD.c\n",
    "    txt = re.sub(r\"\\bmr\\.|\\bjr\\.|\\bms\\.|\\bdr\\.|\\besq\\.|\\bhon\\.|\\bmrs\\.|\\bprof\\.|\\brev\\.|\\bsr\\.|\\bst\\.|\\bno\\.\", '', txt) # titles and common abbreviations\n",
    "    txt = re.sub(r'\\b[a-z]\\.', '', txt) #  L.\n",
    "    txt = re.sub(r'(\\w+)?\\.\\w+', '', txt) # .net .123 www.123\n",
    "    txt = re.sub(r'[\\$\\%\\d]+', '', txt) # remove all $/%/numbers\n",
    "    # final clean format\n",
    "    txt = re.sub(r'[\\.\\:\\;]', '.', txt) # standardize all sentence separators\n",
    "    txt = re.sub(r'( ?\\. ?)+', '. ', txt) # replace consecutive sentence separators\n",
    "    txt = re.sub(r' +', ' ', txt) # replace consecutive spaces\n",
    "    txt = re.sub(r'( ?, ?)+', ', ', txt) # replace consecutive \",\"\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca82c76a",
   "metadata": {
    "_cell_guid": "191e4e00-b28c-4e8b-9aa0-aa264bfe5f35",
    "_uuid": "18ce43d3-d9d5-47ee-86c3-d2d166b6f182",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-05-13T12:23:55.759081Z",
     "iopub.status.busy": "2022-05-13T12:23:55.758063Z",
     "iopub.status.idle": "2022-05-13T12:23:55.823874Z",
     "shell.execute_reply": "2022-05-13T12:23:55.824463Z",
     "shell.execute_reply.started": "2022-05-06T04:16:00.250685Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.110952,
     "end_time": "2022-05-13T12:23:55.824662",
     "exception": false,
     "start_time": "2022-05-13T12:23:55.713710",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Item extraction Regex patterns\n",
    "'''\n",
    "\n",
    "# function to convert txt to re pattern allowing any | between characters\n",
    "def w(txt):\n",
    "    txt = r''.join([x + r'\\|?' for x in list(txt)])\n",
    "    return txt\n",
    "\n",
    "def wu(txt):\n",
    "    txt = r''.join([x + r'\\|?' for x in list(txt)])\n",
    "    return r'(?:' + txt + r'|' + txt.upper() + r')'\n",
    "\n",
    "def s(x='.'):\n",
    "    return x + r'{0,5}'\n",
    "\n",
    "# defining search patterns\n",
    "item_ptrn1 = dict()\n",
    "item_ptrn1['item_1'] = rf\"\\|(?:{wu('Item')}{s()}1{s()}){w('Business')}{s('[^a-z]')}\\|\"\n",
    "item_ptrn1['item_1a'] = rf\"\\|(?:{wu('Item')}{s()}{wu('1a')}{s()}){w('Risk')}{s()}{w('Factors')}{s()}\\|\"\n",
    "item_ptrn1['item_1b'] = rf\"\\|(?:{wu('Item')}{s()}{wu('1b')}{s()}){w('Unresolved')}{s()}(?:{w('Staff')}|{w('SEC')}){s()}{w('Comment')}{s()}\\|\"\n",
    "item_ptrn1['item_2'] = rf\"\\|(?:{wu('Item')}{s()}2{s()}){w('Properties')}{s()}\\|\"\n",
    "item_ptrn1['item_3'] = rf\"\\|(?:{wu('Item')}{s()}3{s()}){w('Legal')}{s()}{w('Proceeding')}{s()}\\|\"\n",
    "item_ptrn1['item_4'] = r'|'.join([rf\"(?:\\|(?:{wu('Item')}{s()}4{s()}){w('Mine')}{s()}{w('Safety')}{s()}{w('Disclosure')}{s()}\\|)\", \n",
    "                                 rf\"(?:\\|(?:{wu('Item')}{s()}4{s()}){w('Submission')}{s()}{w('f')}{s()}{w('Matter')}{s()}{w('o')}{s()}{wu('a')}{s()}{w('Vote')}{s()}{w('f')}{s()}{w('Security')}{s()}{w('Holder')}{s()}\\|)\",\n",
    "                                 rf\"(?:\\|(?:{wu('Item')}{s()}4{s()})(?:{w('Removed')}{s()}{w('nd')}{s()})?{w('Reserved')}{s()}\\|)\"])\n",
    "item_ptrn1['item_5'] = rf\"\\|(?:{wu('Item')}{s()}5{s()}){w('Market')}{s()}{w('or')}{s()}{w('Registrant')}{s()}{w('Common')}{s()}{w('Equit')}(?:{w('y')}|{w('ies')}){s()}{w('Related')}{s()}{w('Stockholder')}{s()}{w('Matter')}{s()}{w('nd')}{s()}{w('Issuer')}{s()}{w('Purchase')}{s()}{w('f')}{s()}{w('Equit')}(?:{w('y')}|{w('ies')}){s()}{w('Securities')}{s()}\\|\"\n",
    "item_ptrn1['item_6'] = rf\"\\|(?:{wu('Item')}{s()}6{s()}){w('Selected')}{s()}(?:{w('Consolidated')}{s()})?{w('Financial')}{s()}{w('Data')}{s()}\\|\"\n",
    "item_ptrn1['item_7'] = r'|'.join([rf\"\\|(?:{wu('Item')}{s()}7{s()}){w('Management')}{s()}{w('Discussion')}{s()}{w('nd')}{s()}{w('Analy')}(?:{w('sis')}|{w('ses')}){s()}{w('f')}{s()}{w('Financial')}{s()}{w('Condition')}{s()}{w('nd')}{s()}{w('Result')}{s()}{w('f')}{s()}{w('Operation')}{s()}\\|\",\n",
    "                                 rf\"\\|(?:{wu('Item')}{s()}7{s()}){w('Management')}{s()}{w('Discussion')}{s()}{w('nd')}{s()}{w('Analy')}(?:{w('sis')}|{w('ses')}){s()}{w('f')}{s()}{w('Result')}{s()}{w('f')}{s()}{w('Operation')}{s()}{w('nd')}{s()}{w('Financial')}{s()}{w('Condition')}{s()}\\|\"])\n",
    "item_ptrn1['item_7a'] = r'|'.join([rf\"\\|(?:{wu('Item')}{s()}{wu('7a')}{s()}){w('Quantitative')}{s()}{w('nd')}{s()}{w('Qualitative')}{s()}{w('Disclosure')}{s()}{w('bout')}{s()}{w('Market')}{s()}{w('Risk')}{s()}\\|\",\n",
    "                                  rf\"\\|(?:{wu('Item')}{s()}{wu('7a')}{s()}){w('Qualitative')}{s()}{w('nd')}{s()}{w('Quantitative')}{s()}{w('Disclosure')}{s()}{w('bout')}{s()}{w('Market')}{s()}{w('Risk')}{s()}\\|\"])\n",
    "item_ptrn1['item_8'] = rf\"\\|(?:{wu('Item')}{s()}8{s()}){w('Financial')}{s()}{w('Statement')}{s()}{w('nd')}{s()}{w('Supplementary')}{s()}{w('Data')}{s()}\\|\"\n",
    "item_ptrn1['item_9'] = rf\"\\|(?:{wu('Item')}{s()}9{s()}){w('Change')}{s()}{w('n')}{s()}{w('nd')}{s()}{w('Disagreement')}{s()}{w('ith')}{s()}{w('Accountant')}{s()}{w('n')}{s()}{w('Accounting')}{s()}{w('nd')}{s()}{w('Financial')}{s()}{w('Disclosure')}{s()}\\|\"\n",
    "item_ptrn1['item_9a'] = rf\"\\|(?:{wu('Item')}{s()}{wu('9a')}{s()}){w('Control')}{s()}{w('nd')}{s()}{w('Procedure')}{s()}\\|\"\n",
    "item_ptrn1['item_9b'] = rf\"\\|(?:{wu('Item')}{s()}{wu('9b')}{s()}){w('Other')}{s()}{w('Information')}{s()}\\|\"\n",
    "item_ptrn1['item_10'] = rf\"\\|(?:{wu('Item')}{s()}10{s()}){w('Director')}{s()}{w('Executive')}{s()}{w('Officer')}{s()}{w('nd')}{s()}{w('Corporate')}{s()}{w('Governance')}{s()}\\|\"\n",
    "item_ptrn1['item_11'] = rf\"\\|(?:{wu('Item')}{s()}11{s()}){w('Executive')}{s()}{w('Compensation')}{s()}\\|\"\n",
    "item_ptrn1['item_12'] = rf\"\\|(?:{wu('Item')}{s()}12{s()}){w('Security')}{s()}{w('Ownership')}{s()}{w('f')}{s()}{w('Certain')}{s()}{w('Beneficial')}{s()}{w('Owner')}{s()}{w('nd')}{s()}{w('Management')}{s()}{w('nd')}{s()}{w('Related')}{s()}{w('Stockholder')}{s()}{w('Matter')}s?{s()}\\|\"\n",
    "item_ptrn1['item_13'] = rf\"\\|(?:{wu('Item')}{s()}13{s()}){w('Certain')}{s()}{w('Relationship')}{s()}{w('nd')}{s()}{w('Related')}{s()}{w('Transaction')}{s()}{w('nd')}{s()}{w('Director')}{s()}{w('Independence')}{s()}\\|\"\n",
    "item_ptrn1['item_14'] = rf\"\\|(?:{wu('Item')}{s()}14{s()}){w('Principal')}{s()}{w('Account')}(?:{w('ant')}|{w('ing')}){s()}{w('Fee')}{s()}{w('nd')}{s()}{w('Service')}{s()}\\|\"\n",
    "item_ptrn1['item_15'] = rf\"\\|(?:{wu('Item')}{s()}15{s()}){w('Exhibits')}{s()}{w('Financial')}{s()}{w('Statement')}{s()}{w('Schedule')}{s()}\\|\"\n",
    "\n",
    "\n",
    "item_ptrn2 = dict()\n",
    "item_ptrn2['item_1'] = rf\"\\|(?:{wu('Item')}{s()}1{s()})?{w('Business')}{s('[^a-z]')}\\|\"\n",
    "item_ptrn2['item_1a'] = rf\"\\|(?:{wu('Item')}{s()}{wu('1a')}{s()})?{w('Risk')}{s()}{w('Factors')}{s()}\\|\"\n",
    "item_ptrn2['item_1b'] = rf\"\\|(?:{wu('Item')}{s()}{wu('1b')}{s()})?{w('Unresolved')}{s()}(?:{w('Staff')}|{w('SEC')}){s()}{w('Comment')}{s()}\\|\"\n",
    "item_ptrn2['item_2'] = rf\"\\|(?:{wu('Item')}{s()}2{s()})?{w('Properties')}{s()}\\|\"\n",
    "item_ptrn2['item_3'] = rf\"\\|(?:{wu('Item')}{s()}3{s()})?{w('Legal')}{s()}{w('Proceeding')}{s()}\\|\"\n",
    "item_ptrn2['item_4'] = r'|'.join([rf\"(?:\\|(?:{wu('Item')}{s()}4{s()})?{w('Mine')}{s()}{w('Safety')}{s()}{w('Disclosure')}{s()}\\|)\", \n",
    "                                 rf\"(?:\\|(?:{wu('Item')}{s()}4{s()})?{w('Submission')}{s()}{w('f')}{s()}{w('Matter')}{s()}{w('o')}{s()}{wu('a')}{s()}{w('Vote')}{s()}{w('f')}{s()}{w('Security')}{s()}{w('Holder')}{s()}\\|)\",\n",
    "                                 rf\"(?:\\|(?:{wu('Item')}{s()}4{s()})(?:{w('Removed')}{s()}{w('nd')}{s()})?{w('Reserved')}{s()}\\|)\"])\n",
    "item_ptrn2['item_5'] = rf\"\\|(?:{wu('Item')}{s()}5{s()})?{w('Market')}{s()}{w('or')}{s()}{w('Registrant')}{s()}{w('Common')}{s()}{w('Equit')}(?:{w('y')}|{w('ies')}){s()}{w('Related')}{s()}{w('Stockholder')}{s()}{w('Matter')}{s()}{w('nd')}{s()}{w('Issuer')}{s()}{w('Purchase')}{s()}{w('f')}{s()}{w('Equit')}(?:{w('y')}|{w('ies')}){s()}{w('Securities')}{s()}\\|\"\n",
    "item_ptrn2['item_6'] = rf\"\\|(?:{wu('Item')}{s()}6{s()})?{w('Selected')}{s()}(?:{w('Consolidated')}{s()})?{w('Financial')}{s()}{w('Data')}{s()}\\|\"\n",
    "item_ptrn2['item_7'] = r'|'.join([rf\"\\|(?:{wu('Item')}{s()}7{s()})?{w('Management')}{s()}{w('Discussion')}{s()}{w('nd')}{s()}{w('Analy')}(?:{w('sis')}|{w('ses')}){s()}{w('f')}{s()}{w('Financial')}{s()}{w('Condition')}{s()}{w('nd')}{s()}{w('Result')}{s()}{w('f')}{s()}{w('Operation')}{s()}\\|\",\n",
    "                                 rf\"\\|(?:{wu('Item')}{s()}7{s()})?{w('Management')}{s()}{w('Discussion')}{s()}{w('nd')}{s()}{w('Analy')}(?:{w('sis')}|{w('ses')}){s()}{w('f')}{s()}{w('Result')}{s()}{w('f')}{s()}{w('Operation')}{s()}{w('nd')}{s()}{w('Financial')}{s()}{w('Condition')}{s()}\\|\"])\n",
    "item_ptrn2['item_7a'] = r'|'.join([rf\"\\|(?:{wu('Item')}{s()}{wu('7a')}{s()})?{w('Quantitative')}{s()}{w('nd')}{s()}{w('Qualitative')}{s()}{w('Disclosure')}{s()}{w('bout')}{s()}{w('Market')}{s()}{w('Risk')}{s()}\\|\",\n",
    "                                  rf\"\\|(?:{wu('Item')}{s()}{wu('7a')}{s()})?{w('Qualitative')}{s()}{w('nd')}{s()}{w('Quantitative')}{s()}{w('Disclosure')}{s()}{w('bout')}{s()}{w('Market')}{s()}{w('Risk')}{s()}\\|\"])\n",
    "item_ptrn2['item_8'] = rf\"\\|(?:{wu('Item')}{s()}8{s()})?{w('Financial')}{s()}{w('Statement')}{s()}{w('nd')}{s()}{w('Supplementary')}{s()}{w('Data')}{s()}\\|\"\n",
    "item_ptrn2['item_9'] = rf\"\\|(?:{wu('Item')}{s()}9{s()})?{w('Change')}{s()}{w('in')}{s()}{w('nd')}{s()}{w('Disagreement')}{s()}{w('ith')}{s()}{w('Accountant')}{s()}{w('n')}{s()}{w('Accounting')}{s()}{w('nd')}{s()}{w('Financial')}{s()}{w('Disclosure')}{s()}\\|\"\n",
    "item_ptrn2['item_9a'] = rf\"\\|(?:{wu('Item')}{s()}{wu('9a')}{s()})?{w('Control')}{s()}{w('nd')}{s()}{w('Procedure')}{s()}\\|\"\n",
    "item_ptrn2['item_9b'] = rf\"\\|(?:{wu('Item')}{s()}{wu('9b')}{s()})?{w('Other')}{s()}{w('Information')}{s()}\\|\"\n",
    "item_ptrn2['item_10'] = rf\"\\|(?:{wu('Item')}{s()}10{s()})?{w('Director')}{s()}{w('Executive')}{s()}{w('Officer')}{s()}{w('nd')}{s()}{w('Corporate')}{s()}{w('Governance')}{s()}\\|\"\n",
    "item_ptrn2['item_11'] = rf\"\\|(?:{wu('Item')}{s()}11{s()})?{w('Executive')}{s()}{w('Compensation')}{s()}\\|\"\n",
    "item_ptrn2['item_12'] = rf\"\\|(?:{wu('Item')}{s()}12{s()})?{w('Security')}{s()}{w('Ownership')}{s()}{w('f')}{s()}{w('Certain')}{s()}{w('Beneficial')}{s()}{w('Owner')}{s()}{w('nd')}{s()}{w('Management')}{s()}{w('nd')}{s()}{w('Related')}{s()}{w('Stockholder')}{s()}{w('Matter')}s?{s()}\\|\"\n",
    "item_ptrn2['item_13'] = rf\"\\|(?:{wu('Item')}{s()}13{s()})?{w('Certain')}{s()}{w('Relationship')}{s()}{w('nd')}{s()}{w('Related')}{s()}{w('Transaction')}{s()}{w('nd')}{s()}{w('Director')}{s()}{w('Independence')}{s()}\\|\"\n",
    "item_ptrn2['item_14'] = rf\"\\|(?:{wu('Item')}{s()}14{s()})?{w('Principal')}{s()}{w('Account')}(?:{w('ant')}|{w('ing')}){s()}{w('Fee')}{s()}{w('nd')}{s()}{w('Service')}{s()}\\|\"\n",
    "item_ptrn2['item_15'] = rf\"\\|(?:{wu('Item')}{s()}15{s()})?{w('Exhibits')}{s()}{w('Financial')}{s()}{w('Statement')}{s()}{w('Schedule')}{s()}\\|\"\n",
    "\n",
    "\n",
    "\n",
    "item_ptrn3 = dict()\n",
    "item_ptrn3['item_1'] = r'|'.join([rf\"\\W{w('Business')}\\W\", \n",
    "                                  rf\"\\W{w('BUSINESS')}\\W\"])\n",
    "item_ptrn3['item_1a'] = r'|'.join([rf\"\\W{w('Risk')}{s()}{w('Factors')}\\W\", \n",
    "                                   rf\"\\W{w('RISK')}{s()}{w('FACTORS')}\\W\"])\n",
    "item_ptrn3['item_1b'] = r'|'.join([rf\"\\W{w('Unresolved')}{s()}(?:{w('Staff')}|{w('SEC')}|{w('Sec')}){s()}{w('Comment')}s?\\W\", \n",
    "                                   rf\"\\W{w('UNRESOLVED')}{s()}(?:{w('STAFF')}|{w('SEC')}){s()}{w('COMMENT')}S?\\W\"])\n",
    "item_ptrn3['item_2'] = r'|'.join([rf\"\\W{w('Properties')}\\W\", \n",
    "                                  rf\"\\W{w('PROPERTIES')}\\W\"])\n",
    "item_ptrn3['item_3'] = r'|'.join([rf\"\\W{w('Legal')}{s()}{w('Proceeding')}s?\", \n",
    "                                  rf\"\\W{w('LEGAL')}{s()}{w('PROCEEDING')}S?\"])\n",
    "item_ptrn3['item_4'] = r'|'.join([rf\"\\W{w('Mine')}{s()}{w('Safety')}{s()}{w('Disclosure')}s?\\W\",\n",
    "                                  rf\"\\W{w('MINE')}{s()}{w('SAFETY')}{s()}{w('DISCLOSURE')}S?\\W\",\n",
    "                                  rf\"\\W(?:{w('Removed')}{s()}(?:{w('A')}|{w('a')}){w('nd')}{s()})?{w('Reserved')}\\W\",\n",
    "                                  rf\"\\W(?:{w('REMOVED')}{s()}{w('AND')}{s()})?{w('RESERVED')}\\W\",\n",
    "                                  rf\"\\W{w('Submission')}{s()}(?:{w('O')}|{w('o')}){w('f')}{s()}{w('Matter')}{s()}(?:{w('T')}|{w('t')}){w('o')}{s()}(?:{w('A')}|{w('a')}){s()}{w('Vote')}{s()}(?:{w('O')}|{w('o')}){w('f')}{s()}{w('Security')}{s()}{w('Holder')}s?\\W\",\n",
    "                                  rf\"\\W{w('SUBMISSION')}{s()}{w('OF')}{s()}{w('MATTER')}{s()}{w('TO')}{s()}{w('A')}{s()}{w('VOTE')}{s()}{w('OF')}{s()}{w('SECURITY')}{s()}{w('HOLDER')}S?\\W\"])\n",
    "item_ptrn3['item_5'] = r'|'.join([rf\"\\W{w('Market')}{s()}(?:{w('F')}|{w('f')}){w('or')}{s()}{w('Registrant')}{s()}{w('Common')}{s()}{w('Equit')}(?:{w('y')}|{w('ies')}){s()}{w('Related')}{s()}{w('Stockholder')}{s()}{w('Matter')}{s()}(?:{w('A')}|{w('a')}){w('nd')}{s()}{w('Issuer')}{s()}{w('Purchase')}{s()}(?:{w('O')}|{w('o')}){w('f')}{s()}{w('Equit')}(?:{w('y')}|{w('ies')}){s()}{w('Securities')}\\W\", \n",
    "                                  rf\"\\W{w('MARKET')}{s()}{w('FOR')}{s()}{w('REGISTRANT')}{s()}{w('COMMON')}{s()}{w('EQUIT')}(?:{w('Y')}|{w('IES')}){s()}{w('RELATED')}{s()}{w('STOCKHOLDER')}{s()}{w('MATTER')}{s()}{w('AND')}{s()}{w('ISSUER')}{s()}{w('PURCHASE')}{s()}{w('OF')}{s()}{w('EQUIT')}(?:{w('Y')}|{w('IES')}){s()}{w('SECURITIES')}\\W\"])\n",
    "item_ptrn3['item_6'] = r'|'.join([rf\"\\W{w('Selected')}{s()}(?:{w('Consolidated')}{s()})?{w('Financial')}{s()}{w('Data')}\\W\", \n",
    "                                  rf\"\\W{w('SELECTED')}{s()}(?:{w('CONSOLIDATED')}{s()})?{w('FINANCIAL')}{s()}{w('DATA')}\\W\"])\n",
    "item_ptrn3['item_7'] = r'|'.join([rf\"\\W{w('Management')}{s()}{w('Discussion')}{s()}(?:{w('A')}|{w('a')}){w('nd')}{s()}{w('Analy')}(?:{w('sis')}|{w('ses')}){s()}(?:{w('O')}|{w('o')}){w('f')}{s()}{w('Financial')}{s()}{w('Condition')}{s()}(?:{w('A')}|{w('a')}){w('nd')}{s()}{w('Result')}{s()}(?:{w('O')}|{w('o')}){w('f')}{s()}{w('Operation')}s?\\W\", \n",
    "                                  rf\"\\W{w('MANAGEMENT')}{s()}{w('DISCUSSION')}{s()}{w('AND')}{s()}{w('ANALY')}(?:{w('SIS')}|{w('SES')}){s()}{w('OF')}{s()}{w('FINANCIAL')}{s()}{w('CONDITION')}{s()}{w('AND')}{s()}{w('RESULT')}{s()}{w('OF')}{s()}{w('OPERATION')}S?\\W\",\n",
    "                                  rf\"\\W{w('Management')}{s()}{w('Discussion')}{s()}(?:{w('A')}|{w('a')}){w('nd')}{s()}{w('Analy')}(?:{w('sis')}|{w('ses')}){s()}(?:{w('O')}|{w('o')}){w('f')}{s()}{w('Result')}{s()}(?:{w('O')}|{w('o')}){w('f')}{s()}{w('Operation')}{s()}(?:{w('A')}|{w('a')}){w('nd')}{s()}{w('Financial')}{s()}{w('Condition')}s?\\W\", \n",
    "                                  rf\"\\W{w('MANAGEMENT')}{s()}{w('DISCUSSION')}{s()}{w('AND')}{s()}{w('ANALY')}(?:{w('SIS')}|{w('SES')}){s()}{w('OF')}{s()}{w('RESULT')}{s()}{w('OF')}{s()}{w('OPERATION')}{s()}{w('AND')}{s()}{w('FINANCIAL')}{s()}{w('CONDITION')}S?\\W\"])\n",
    "item_ptrn3['item_7a'] = '|'.join([rf\"\\W{w('Quantitative')}{s()}(?:{w('A')}|{w('a')}){w('nd')}{s()}{w('Qualitative')}{s()}{w('Disclosure')}{s()}(?:{w('A')}|{w('a')}){w('bout')}{s()}{w('Market')}{s()}{w('Risk')}s?\\W\",\n",
    "                                  rf\"\\W{w('QUANTITATIVE')}{s()}{w('AND')}{s()}{w('QUALITATIVE')}{s()}{w('DISCLOSURE')}{s()}{w('ABOUT')}{s()}{w('MARKET')}{s()}{w('RISK')}S?\\W\",\n",
    "                                  rf\"\\W{w('Qualitative')}{s()}(?:{w('A')}|{w('a')}){w('nd')}{s()}{w('Quantitative')}{s()}{w('Disclosure')}{s()}(?:{w('A')}|{w('a')}){w('bout')}{s()}{w('Market')}{s()}{w('Risk')}s?\\W\",\n",
    "                                  rf\"\\W{w('QUALITATIVE')}{s()}{w('AND')}{s()}{w('QUANTITATIVE')}{s()}{w('DISCLOSURE')}{s()}{w('ABOUT')}{s()}{w('MARKET')}{s()}{w('RISK')}S?\\W\"])\n",
    "item_ptrn3['item_8'] = r'|'.join([rf\"\\W{w('Financial')}{s()}{w('Statement')}s?{s()}(?:{w('A')}|{w('a')}){w('nd')}{s()}{w('Supplementary')}{s()}{w('Data')}\\W\",\n",
    "                                  rf\"\\W{w('FINANCIAL')}{s()}{w('STATEMENT')}S?{s()}{w('AND')}{s()}{w('SUPPLEMENTARY')}{s()}{w('DATA')}\\W\"])\n",
    "item_ptrn3['item_9'] = r'|'.join([rf\"\\W{w('Change')}{s()}(?:{w('I')}|{w('i')}){w('n')}{s()}(?:{w('A')}|{w('a')}){w('nd')}{s()}{w('Disagreement')}{s()}(?:{w('W')}|{w('w')}){w('ith')}{s()}{w('Accountant')}{s()}(?:{w('O')}|{w('o')}){w('n')}{w('Accounting')}{s()}(?:{w('A')}|{w('a')}){w('nd')}{s()}{w('Financial')}{s()}{w('Disclosure')}s?\\W\",\n",
    "                                  rf\"\\W{w('CHANGE')}{s()}{w('IN')}{s()}{w('AND')}{s()}{w('DISAGREEMENT')}{s()}{w('WITH')}{s()}{w('ACCOUNTANT')}{s()}{w('ON')}{w('ACCOUNTING')}{s()}{w('AND')}{s()}{w('FINANCIAL')}{s()}{w('DISCLOSURE')}S?\\W\"])\n",
    "item_ptrn3['item_9a'] = r'|'.join([rf\"\\W{w('Control')}s?{s()}(?:{w('A')}|{w('a')}){w('nd')}{s()}{w('Procedure')}s?\\W\",\n",
    "                                   rf\"\\W{w('CONTROL')}S?{s()}{w('AND')}{s()}{w('PROCEDURE')}S?\\W\"])\n",
    "item_ptrn3['item_9b'] = r'|'.join([rf\"\\W{w('Other')}{s()}{w('Information')}\\W\",\n",
    "                                   rf\"\\W{w('OTHER')}{s()}{w('INFORMATION')}\\W\"])\n",
    "item_ptrn3['item_10'] = r'|'.join([rf\"\\W{w('Director')}{s()}{w('Executive')}{s()}{w('Officer')}{s()}(?:{w('A')}|{w('a')}){w('nd')}{s()}{w('Corporate')}{s()}{w('Governance')}s?\\W\",\n",
    "                                   rf\"\\W{w('DIRECTOR')}{s()}{w('EXECUTIVE')}{s()}{w('OFFICER')}{s()}{w('AND')}{s()}{w('CORPORATE')}{s()}{w('GOVERNANCE')}S?\\W\"])\n",
    "item_ptrn3['item_11'] = r'|'.join([rf\"\\W{w('Executive')}{s()}{w('Compensation')}s?\\W\",\n",
    "                                   rf\"\\W{w('EXECUTIVE')}{s()}{w('COMPENSATION')}S?\\W\"])\n",
    "item_ptrn3['item_12'] = r'|'.join([rf\"\\W{w('Security')}{s()}{w('Ownership')}{s()}(?:{w('O')}|{w('o')}){w('f')}{s()}{w('Certain')}{s()}{w('Beneficial')}{s()}{w('Owner')}{s()}(?:{w('A')}|{w('a')}){w('nd')}{s()}{w('Management')}{s()}(?:{w('A')}|{w('a')}){w('nd')}{s()}{w('Related')}{s()}{w('Stockholder')}{s()}{w('Matter')}s?\\W\",\n",
    "                                   rf\"\\W{w('SECURITY')}{s()}{w('OWNERSHIP')}{s()}{w('OF')}{s()}{w('CERTAIN')}{s()}{w('BENEFICIAL')}{s()}{w('OWNER')}{s()}{w('AND')}{s()}{w('MANAGEMENT')}{s()}{w('AND')}{s()}{w('RELATED')}{s()}{w('STOCKHOLDER')}{s()}{w('MATTER')}S?\\W\"])\n",
    "item_ptrn3['item_13'] = r'|'.join([rf\"\\W{w('Certain')}{s()}{w('Relationship')}{s()}(?:{w('A')}|{w('a')}){w('nd')}{s()}{w('Related')}{s()}{w('Transaction')}{s()}(?:{w('A')}|{w('a')}){w('nd')}{s()}{w('Director')}{s()}{w('Independence')}\\W\",\n",
    "                                   rf\"\\W{w('CERTAIN')}{s()}{w('RELATIONSHIP')}{s()}{w('AND')}{s()}{w('RELATED')}{s()}{w('TRANSACTION')}{s()}{w('AND')}{s()}{w('DIRECTOR')}{s()}{w('INDEPENDENCE')}\\W\"])\n",
    "item_ptrn3['item_14'] = r'|'.join([rf\"\\W{w('Principal')}{s()}{w('Account')}(?:{w('ant')}|{w('ing')}){s()}{w('Fee')}{s()}(?:{w('A')}|{w('a')}){w('nd')}{s()}{w('Service')}s?\\W\",\n",
    "                                   rf\"\\W{w('PRINCIPAL')}{s()}{w('ACCOUNT')}(?:{w('ANT')}|{w('IND')}){s()}{w('FEE')}{s()}{w('AND')}{s()}{w('SERVICE')}S?\\W\"])\n",
    "item_ptrn3['item_15'] = r'|'.join([rf\"\\W{w('Exhibits')}{s()}{w('Financial')}{s()}{w('Statement')}{s()}{w('Schedule')}s?\\W\",\n",
    "                                   rf\"\\W{w('EXHIBITS')}{s()}{w('FINANCIAL')}{s()}{w('STATEMENT')}{s()}{w('SCHEDULE')}S?\\W\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42989d16",
   "metadata": {
    "_cell_guid": "6450cc8e-7a09-45cb-a72d-53e86ea10b48",
    "_uuid": "4934fe32-114c-4a0f-89b8-8e1c36ff1c85",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-05-13T12:23:55.912719Z",
     "iopub.status.busy": "2022-05-13T12:23:55.911951Z",
     "iopub.status.idle": "2022-05-13T12:23:55.933145Z",
     "shell.execute_reply": "2022-05-13T12:23:55.933654Z",
     "shell.execute_reply.started": "2022-05-06T04:16:00.3305Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.066412,
     "end_time": "2022-05-13T12:23:55.933845",
     "exception": false,
     "start_time": "2022-05-13T12:23:55.867433",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7 s, sys: 1e+03 ns, total: 8 s\n",
      "Wall time: 11.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\"\"\"\n",
    "Given a document, extract start and end position of each Item\n",
    "\"\"\"\n",
    "\n",
    "def dedup_pos(pos):\n",
    "    return list(pd.DataFrame({0:[x[0] for x in pos], 1:[x[1] for x in pos]}).drop_duplicates(subset=[0]).to_records(index=False))\n",
    "\n",
    "def find_item_pos(doc, log_mode=False):\n",
    "    item_pos = []\n",
    "    \n",
    "    # loop througn all items\n",
    "    for item in item_ptrn1:\n",
    "        \n",
    "        # pattern 1 (normal + upper)\n",
    "        pos = [(m.start(), m.end()) for m in re.finditer(item_ptrn1[item], doc)] + [(m.start(), m.end()) for m in re.finditer(item_ptrn1[item].upper(), doc)]\n",
    "        pos = dedup_pos(pos)\n",
    "        log(f'[{item}] After attempt 1 yielded {len(pos)} matches') if log_mode==True else None\n",
    "\n",
    "        # pattern 2 (\"Item\" as optional, normal + upper)\n",
    "        if len(pos) == 0 or (len(pos) == 1 and len(re.findall(rf\"{w('table')}{s()}{w('of')}{s()}{w('content')}\", doc.lower())) > 0 and pos[0][0] < 7000):\n",
    "            pos = pos + [(m.start(), m.end()) for m in re.finditer(item_ptrn2[item], doc)] + [(m.start(), m.end()) for m in re.finditer(item_ptrn2[item].upper(), doc)]\n",
    "            pos = dedup_pos(pos)\n",
    "            log(f'[{item}] After attempt 2 yielded {len(pos)} matches') if log_mode==True else None\n",
    "\n",
    "        # pattern 3\n",
    "        if len(pos) == 0 or (len(pos) == 1 and len(re.findall(rf\"{w('table')}{s()}{w('of')}{s()}{w('content')}\", doc.lower())) > 0 and pos[0][0] < 7000):\n",
    "            pos = pos + [(m.start(), m.end()) for m in re.finditer(item_ptrn3[item], doc)]\n",
    "            pos = dedup_pos(pos)\n",
    "            log(f'[{item}] After attempt 3 yielded {len(pos)} matches') if log_mode==True else None\n",
    "\n",
    "\n",
    "        # remove first entry due to table of contents\n",
    "        if len(pos) >= 2  \\\n",
    "        and len(re.findall(rf\"{w('table')}{s()}{w('of')}{s()}{w('content')}\", doc.lower())) > 0 \\\n",
    "        and pos[0][0] < 6000 \\\n",
    "        and item != 'item_1':\n",
    "            pos = pos[1:]\n",
    "            log(f'[{item}] Removed first result due to Table of Contents') if log_mode==True else None\n",
    "\n",
    "        # remove occurrance due to references\n",
    "        pos_filtered = []\n",
    "        for p in pos:\n",
    "            match = doc[p[0]:p[1]]\n",
    "            pre = doc[p[0]-20:p[0]].lower()\n",
    "            suf = doc[p[1]:p[1]+20].lower()\n",
    "            log(f'[{item}] pos {p} : <<{pre}....{match}....{suf}>>') if log_mode==True else None\n",
    "            pre_ptrn = r\"\"\"(\\W\"$|\\W$|('s\\W)$|\\Wsee\\W$|\\Win\\W$|\\Wthe\\W$|\\Wour\\W$|\\Wthis\\W$|\\Wwithin\\W$|\\Wherein\\W$|\\Wrefer to\\W$|\\Wreferring\\W$)\"\"\"\n",
    "            suf_ptrn = r\"\"\"(^\\Wshould\\W|^\\Wshall\\W|^\\Wmust\\W|^\\Wwas\\W|^\\Wwere\\W|^\\Whas\\W|^\\Whad\\W|^\\Wis\\W|^\\Ware\\W)\"\"\"\n",
    "            if re.search(pre_ptrn, pre) or re.search(suf_ptrn, suf):\n",
    "                log(f'[{item}] removed the above match') if log_mode==True else None\n",
    "            else:\n",
    "                pos_filtered.append(p)\n",
    "        pos = pos_filtered.copy()\n",
    "\n",
    "        # save position as dataframe\n",
    "        pos = pd.DataFrame({'item':[item]*len(pos), 'pos_start':[x[0] for x in pos]})\n",
    "        item_pos.append(pos)\n",
    "\n",
    "    # combine positions for all items\n",
    "    item_pos = pd.concat(item_pos).sort_values('pos_start').reset_index(drop=True)\n",
    "    # define ending position\n",
    "    item_pos['pos_end'] = item_pos.pos_start.shift(-1).fillna(len(doc))\n",
    "    # define length\n",
    "    item_pos['len'] = item_pos.pos_end - item_pos.pos_start\n",
    "    # for each item, select the match with longest length\n",
    "    item_pos = item_pos.sort_values(['item','len','pos_start'], ascending=[1,0,0]).drop_duplicates(subset=['item']).sort_values('pos_start')\n",
    "    item_pos = pd.concat([item_pos[item_pos.item==item][['pos_start','pos_end']].reset_index(drop=True).rename(columns={'pos_start':f'{item}_pos_start','pos_end':f'{item}_pos_end'}) for item in item_ptrn1], axis=1)\n",
    "    # fillna with zero\n",
    "    item_pos = item_pos.fillna(0).astype(int)\n",
    "    # if item_pos is empty due to no item found, put all zeros as a row\n",
    "    if item_pos.shape[0] == 0:\n",
    "        item_pos.loc[0,:] = [0] * 2 * len(item_ptrn1)\n",
    "    # record the full document length\n",
    "    item_pos['full_doc_len'] = len(doc)\n",
    "    # check if non empty df is returned\n",
    "    assert item_pos.shape[0]==1\n",
    "    return item_pos\n",
    "\n",
    "\n",
    "# function to sample check item extraction quality\n",
    "def show_item(doc_dict):\n",
    "    n = 100\n",
    "    for item in item_ptrn1:\n",
    "        print(f'{item}: {doc_dict[item][:n]}........{doc_dict[item][-n:]}')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3a5c0e18",
   "metadata": {
    "_cell_guid": "4a77cbba-7b48-4cff-9cb2-6efe3f1f865b",
    "_uuid": "598ce949-5938-45e2-9e2c-43ac2aa52070",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-05-13T12:23:56.025256Z",
     "iopub.status.busy": "2022-05-13T12:23:56.022623Z",
     "iopub.status.idle": "2022-05-13T12:23:56.027399Z",
     "shell.execute_reply": "2022-05-13T12:23:56.028086Z",
     "shell.execute_reply.started": "2022-05-06T04:16:00.355753Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.052162,
     "end_time": "2022-05-13T12:23:56.028267",
     "exception": false,
     "start_time": "2022-05-13T12:23:55.976105",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# urls = ['https://www.sec.gov/Archives/edgar/data/1166691/000119312508034239/d10k.htm',\n",
    "#        'https://www.sec.gov/Archives/edgar/data/922224/000092222411000029/form10k.htm',\n",
    "#        'https://www.sec.gov/Archives/edgar/data/1283699/000119312511051403/d10k.htm']\n",
    "# docs = {}\n",
    "# for i in range(len(urls)):\n",
    "\n",
    "#     url = urls[i]\n",
    "#     doc_id = i\n",
    "#     txt = requests.get(url, headers={\"user-agent\": f\"chan_tai_man_{int(float(np.random.rand(1)) * 1e7)}@gmail.com\"}).text\n",
    "\n",
    "#     # clean doc, extract items\n",
    "#     txt = soup = BeautifulSoup(txt, 'lxml').get_text('|', strip=True)\n",
    "#     txt = clean_doc1(txt)\n",
    "#     item_pos = find_item_pos(txt)\n",
    "#     doc_dict = {}\n",
    "#     doc_dict['full'] = txt[item_pos.iloc[0]['item_1_pos_start'] :]\n",
    "#     for item in item_ptrn1:\n",
    "#         doc_dict[item] = txt[item_pos.iloc[0][f'{item}_pos_start'] : item_pos.iloc[0][f'{item}_pos_end']]\n",
    "#     for x in doc_dict:\n",
    "#         doc_dict[x] = clean_doc2(doc_dict[x])\n",
    "#     docs[doc_id] = doc_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fd40fe",
   "metadata": {
    "_cell_guid": "c63e307b-f1a2-4526-a9ed-c4b579aa6e6e",
    "_uuid": "5caf7da6-8314-45d9-84e1-ba33d7561ba2",
    "papermill": {
     "duration": 0.041367,
     "end_time": "2022-05-13T12:23:56.111157",
     "exception": false,
     "start_time": "2022-05-13T12:23:56.069790",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Signal Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bc22095a",
   "metadata": {
    "_cell_guid": "5016e34c-4c12-4d01-8e33-f2d7e9ac5f02",
    "_uuid": "3ba7614e-ea8e-48aa-9485-afe1d378c741",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-05-13T12:23:56.202073Z",
     "iopub.status.busy": "2022-05-13T12:23:56.201033Z",
     "iopub.status.idle": "2022-05-13T12:23:56.207572Z",
     "shell.execute_reply": "2022-05-13T12:23:56.208043Z",
     "shell.execute_reply.started": "2022-05-06T04:16:00.364359Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.054692,
     "end_time": "2022-05-13T12:23:56.208219",
     "exception": false,
     "start_time": "2022-05-13T12:23:56.153527",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDownload NLP pretrained models\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Download NLP pretrained models\n",
    "'''\n",
    "\n",
    "# if mode in ['full','gpu']:\n",
    "#     !pip install sentence-transformers\n",
    "#     from sentence_transformers import SentenceTransformer\n",
    "#     st_model = SentenceTransformer('all-MiniLM-L6-v2', device='cuda')\n",
    "\n",
    "#     from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n",
    "#     import torch\n",
    "#     fb_tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
    "#     fb_model = AutoModelForSequenceClassification.from_pretrained(\"ProsusAI/finbert\")\n",
    "#     fb_model = fb_model.to(\"cuda:0\")\n",
    "\n",
    "# if mode in ['full','wv']:\n",
    "# import gensim.downloader as api\n",
    "# for i in range(20):\n",
    "#     try:\n",
    "#         wv = api.load('word2vec-google-news-300')\n",
    "#         break\n",
    "#     except:\n",
    "#         continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "685d0f67",
   "metadata": {
    "_cell_guid": "067d3956-0540-41c3-bbd7-2c653e357f78",
    "_uuid": "02d633df-6cac-4481-829e-a2447d8523aa",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-05-13T12:23:56.297582Z",
     "iopub.status.busy": "2022-05-13T12:23:56.296767Z",
     "iopub.status.idle": "2022-05-13T12:23:56.299742Z",
     "shell.execute_reply": "2022-05-13T12:23:56.299179Z",
     "shell.execute_reply.started": "2022-05-06T04:16:00.379735Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.048943,
     "end_time": "2022-05-13T12:23:56.299893",
     "exception": false,
     "start_time": "2022-05-13T12:23:56.250950",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# '''\n",
    "# Loughran and McDonalds Master Dictionary\n",
    "# '''\n",
    "# # load Loughran and McDonalds Master Dictionary (2020)\n",
    "# master_dict = pd.read_csv('../input/loughranmcdonald-masterdictionary-2020/LoughranMcDonald_MasterDictionary_2020.csv')\n",
    "# master_dict.columns = ['_'.join([y.lower() for y in x.split()]) for x in master_dict.columns]\n",
    "# master_dict.word = master_dict.word.str.lower()\n",
    "\n",
    "# # extract specific word lists\n",
    "# negative_word_list = master_dict.loc[lambda x: x.negative!=0].word.tolist()\n",
    "# positive_word_list = master_dict.loc[lambda x: x.positive!=0].word.tolist()\n",
    "# uncertainty_word_list = master_dict.loc[lambda x: x.uncertainty!=0].word.tolist()\n",
    "# litigious_word_list = master_dict.loc[lambda x: x.litigious!=0].word.tolist()\n",
    "# strong_modal_word_list = master_dict.loc[lambda x: x.strong_modal!=0].word.tolist()\n",
    "# weak_modal_word_list = master_dict.loc[lambda x: x.weak_modal!=0].word.tolist()\n",
    "# constraining_word_list = master_dict.loc[lambda x: x.constraining!=0].word.tolist()\n",
    "# complexity_word_list = master_dict.loc[lambda x: x.complexity!=0].word.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9e40b07d",
   "metadata": {
    "_cell_guid": "6c740978-831d-44a4-aaed-1816802e6a50",
    "_uuid": "f2990648-b7f6-4c58-a196-48c0ecf02ab7",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-05-13T12:23:56.387980Z",
     "iopub.status.busy": "2022-05-13T12:23:56.387273Z",
     "iopub.status.idle": "2022-05-13T12:23:56.397821Z",
     "shell.execute_reply": "2022-05-13T12:23:56.398373Z",
     "shell.execute_reply.started": "2022-05-06T04:16:00.392452Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.05583,
     "end_time": "2022-05-13T12:23:56.398573",
     "exception": false,
     "start_time": "2022-05-13T12:23:56.342743",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Change in length\n",
    "'''\n",
    "# full doc\n",
    "def gen_feat_ch_full_len(docs):\n",
    "    feat = pd.Series([len(doc_dict['full']) for doc_dict in docs.values()])\n",
    "    feat = np.log(feat).diff()\n",
    "    feat = feat * -1\n",
    "    return feat.rename('feat_ch_full_len')\n",
    "\n",
    "# Item 1A - Risk Factors\n",
    "def gen_feat_ch_item_1a_len(docs):\n",
    "    feat = pd.Series([len(doc_dict['item_1a']) for doc_dict in docs.values()])\n",
    "    feat = np.log(feat).diff()\n",
    "    feat = feat * -1\n",
    "    return feat.rename('feat_ch_item_1a_len')\n",
    "\n",
    "# Item 1B - Unresolved Staff Comments\n",
    "def gen_feat_ch_item_1b_len(docs):\n",
    "    feat = pd.Series([len(doc_dict['item_1b']) for doc_dict in docs.values()])\n",
    "    feat = np.log(feat).diff()\n",
    "    feat = feat * -1\n",
    "    return feat.rename('feat_ch_item_1b_len')\n",
    "\n",
    "# Item 3 - Legal Proceedings\n",
    "def gen_feat_ch_item_3_len(docs):\n",
    "    feat = pd.Series([len(doc_dict['item_3']) for doc_dict in docs.values()])\n",
    "    feat = np.log(feat).diff()\n",
    "    feat = feat * -1\n",
    "    return feat.rename('feat_ch_item_3_len')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5f5cba34",
   "metadata": {
    "_cell_guid": "c6ec9ef3-4072-4056-b16c-0ceb9d73d1e6",
    "_uuid": "168be8c6-4c03-4cd3-a820-2588d0c6d47f",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-05-13T12:23:56.488522Z",
     "iopub.status.busy": "2022-05-13T12:23:56.487789Z",
     "iopub.status.idle": "2022-05-13T12:23:56.502647Z",
     "shell.execute_reply": "2022-05-13T12:23:56.503398Z",
     "shell.execute_reply.started": "2022-05-06T04:16:00.41261Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.061371,
     "end_time": "2022-05-13T12:23:56.503629",
     "exception": false,
     "start_time": "2022-05-13T12:23:56.442258",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Document Similarity\n",
    "'''\n",
    "# full doc, cosine similarity, 1 gram\n",
    "def gen_feat_full_cos_1gram(docs):\n",
    "    doc_list = [doc_dict['full'] for doc_dict in docs.values()]\n",
    "    tf_vectors = global_tfidf_1g.transform(doc_list)\n",
    "    feat = pd.Series([cosine_similarity(tf_vectors[i-1:i+1,:])[0][1] if i > 0 else np.NaN for i in range(len(doc_list))])\n",
    "    return feat.rename('feat_full_cos_1gram')\n",
    "\n",
    "# full doc, cosine similarity, 2 gram\n",
    "def gen_feat_full_cos_2gram(docs):\n",
    "    doc_list = [doc_dict['full'] for doc_dict in docs.values()]\n",
    "    tf_vectors = global_tfidf_2g.transform(doc_list)\n",
    "    feat = pd.Series([cosine_similarity(tf_vectors[i-1:i+1,:])[0][1] if i > 0 else np.NaN for i in range(len(doc_list))])\n",
    "    return feat.rename('feat_full_cos_2gram')\n",
    "\n",
    "# full doc, jaccard similarity, 1 gram\n",
    "def gen_feat_full_jac_1gram(docs):\n",
    "    doc_list = [doc_dict['full'] for doc_dict in docs.values()]\n",
    "    vectorizer = CountVectorizer(ngram_range=(1,1), binary=True, token_pattern=r\"(?u)\\b[a-z]{3,}\\b\")\n",
    "    tf_vectors = vectorizer.fit_transform(doc_list)\n",
    "    feat = pd.Series([jaccard_score(tf_vectors[i-1,:].toarray().flatten(), tf_vectors[i,:].toarray().flatten()) if i > 0 else np.NaN for i in range(len(doc_list))])\n",
    "    return feat.rename('feat_full_jac_1gram')\n",
    "\n",
    "# full doc, jaccard similarity, 2 gram\n",
    "def gen_feat_full_jac_2gram(docs):\n",
    "    doc_list = [doc_dict['full'] for doc_dict in docs.values()]\n",
    "    vectorizer = CountVectorizer(ngram_range=(1,2), binary=True, token_pattern=r\"(?u)\\b[a-z]{3,}\\b\")\n",
    "    tf_vectors = vectorizer.fit_transform(doc_list)\n",
    "    feat = pd.Series([jaccard_score(tf_vectors[i-1,:].toarray().flatten(), tf_vectors[i,:].toarray().flatten()) if i > 0 else np.NaN for i in range(len(doc_list))])\n",
    "    return feat.rename('feat_full_jac_2gram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0412dd76",
   "metadata": {
    "_cell_guid": "261de8b4-d8e3-4a2f-b9a2-94ded0b983ff",
    "_uuid": "342e62ad-454b-4dde-b6ce-a399cf51ef08",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-05-13T12:23:56.592716Z",
     "iopub.status.busy": "2022-05-13T12:23:56.591925Z",
     "iopub.status.idle": "2022-05-13T12:23:56.608211Z",
     "shell.execute_reply": "2022-05-13T12:23:56.608781Z",
     "shell.execute_reply.started": "2022-05-06T04:16:00.432177Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.063612,
     "end_time": "2022-05-13T12:23:56.608964",
     "exception": false,
     "start_time": "2022-05-13T12:23:56.545352",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Dictionary based sentiment (Loughran and McDonald)\n",
    "'''\n",
    "# net postive words change in proportion\n",
    "def gen_feat_lm_postive(docs):\n",
    "    doc_list = [doc_dict['full'] for doc_dict in docs.values()]\n",
    "    doc_len = pd.Series([len(x) for x in doc_list])\n",
    "    vectorizer = CountVectorizer(ngram_range=(1,1), binary=False, token_pattern=r\"(?u)\\b[a-z]{3,}\\b\").fit(doc_list)\n",
    "    tf_vectors = vectorizer.transform(doc_list)\n",
    "    pos_target_cols = [vectorizer.vocabulary_[x] for x in positive_word_list if x in list(vectorizer.vocabulary_.keys())]\n",
    "    neg_target_cols = [vectorizer.vocabulary_[x] for x in negative_word_list if x in list(vectorizer.vocabulary_.keys())]\n",
    "    feat = pd.Series([(tf_vectors[i,pos_target_cols].sum() - tf_vectors[i,neg_target_cols].sum()) / doc_len[i] for i in range(len(doc_list))]).diff()\n",
    "    return feat.rename('feat_lm_postive')\n",
    "\n",
    "# uncertainty words change in proportion\n",
    "def gen_feat_lm_uncertainty(docs):\n",
    "    doc_list = [doc_dict['full'] for doc_dict in docs.values()]\n",
    "    doc_len = pd.Series([len(x) for x in doc_list])\n",
    "    vectorizer = CountVectorizer(ngram_range=(1,1), binary=False, token_pattern=r\"(?u)\\b[a-z]{3,}\\b\").fit(doc_list)\n",
    "    tf_vectors = vectorizer.transform(doc_list)\n",
    "    target_cols = [vectorizer.vocabulary_[x] for x in uncertainty_word_list if x in list(vectorizer.vocabulary_.keys())]\n",
    "    feat = pd.Series([tf_vectors[i,target_cols].sum() / doc_len[i] for i in range(len(doc_list))]).diff()\n",
    "    feat = feat * -1\n",
    "    return feat.rename('feat_lm_uncertainty')\n",
    "\n",
    "# uncertainty words change in proportion\n",
    "def gen_feat_lm_litigious(docs):\n",
    "    doc_list = [doc_dict['full'] for doc_dict in docs.values()]\n",
    "    doc_len = pd.Series([len(x) for x in doc_list])\n",
    "    vectorizer = CountVectorizer(ngram_range=(1,1), binary=False, token_pattern=r\"(?u)\\b[a-z]{3,}\\b\").fit(doc_list)\n",
    "    tf_vectors = vectorizer.transform(doc_list)\n",
    "    target_cols = [vectorizer.vocabulary_[x] for x in litigious_word_list if x in list(vectorizer.vocabulary_.keys())]\n",
    "    feat = pd.Series([tf_vectors[i,target_cols].sum() / doc_len[i] for i in range(len(doc_list))]).diff()\n",
    "    feat = feat * -1\n",
    "    return feat.rename('feat_lm_litigious')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7782673d",
   "metadata": {
    "_cell_guid": "b4f1619f-27c1-415e-8cb4-5f00fd54143c",
    "_uuid": "a5f3b4c3-bfed-4e18-bb45-4f2049925be9",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-05-13T12:23:56.694726Z",
     "iopub.status.busy": "2022-05-13T12:23:56.693917Z",
     "iopub.status.idle": "2022-05-13T12:23:56.722609Z",
     "shell.execute_reply": "2022-05-13T12:23:56.721891Z",
     "shell.execute_reply.started": "2022-05-06T04:16:00.45381Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.072743,
     "end_time": "2022-05-13T12:23:56.722756",
     "exception": false,
     "start_time": "2022-05-13T12:23:56.650013",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Sentence encoding\n",
    "'''\n",
    "def gen_feat_sen_enc(docs):\n",
    "    doc_list = [doc_dict['full'] for doc_dict in docs.values()]\n",
    "    vecs = []\n",
    "    for doc in doc_list:\n",
    "        sen_list = [x for x in tokenize.sent_tokenize(doc) if len(x)>=30 and len(x)<=1000 and re.match(r'[a-z]', x)]\n",
    "        vecs.append(st_model.encode(sentences=sen_list, show_progress_bar=False).mean(axis=0).flatten())\n",
    "    vecs = np.concatenate(vecs).reshape(-1, vecs[0].shape[0])\n",
    "    feat = pd.Series([cosine_similarity(vecs[i-1:i+1,:])[0][1] if i > 0 else np.NaN for i in range(len(doc_list))])\n",
    "    return feat.rename('feat_sen_enc')\n",
    "\n",
    "\n",
    "'''\n",
    "Finbert Sentiment on Item 1A & 7\n",
    "'''\n",
    "def gen_feat_item_sentiment(docs):\n",
    "    doc_list = [doc_dict['item_1a'] + '.' + doc_dict['item_7'] for doc_dict in docs.values()]\n",
    "    sentiment = []\n",
    "    for doc in doc_list:\n",
    "        sen_list = [x for x in tokenize.sent_tokenize(doc) if len(x)>=30 and len(x)<=1000 and re.match(r'[a-z]', x)]\n",
    "        if len(sen_list)==0:\n",
    "            sentiment.append(np.NaN)\n",
    "            continue\n",
    "        batch_size = 8\n",
    "        n_batch = math.ceil(len(sen_list)/batch_size)\n",
    "        sentiment_sum = 0\n",
    "        for i in range(n_batch):\n",
    "            inputs = fb_tokenizer(sen_list[batch_size*i:batch_size*(i+1)], padding=True, truncation=True, return_tensors='pt').to(\"cuda:0\")\n",
    "            with torch.no_grad():\n",
    "                outputs = fb_model(**inputs)\n",
    "            sentiment_sum += float(torch.nn.functional.softmax(outputs.logits, dim=-1)[:,0].sum())\n",
    "            torch.cuda.empty_cache()\n",
    "        sentiment.append(sentiment_sum / len(sen_list))\n",
    "    feat = pd.Series(sentiment).ffill().diff()\n",
    "    return feat.rename('feat_item_sentiment')\n",
    "\n",
    "\n",
    "'''\n",
    "Finbert Sentiment on Forward-Looking Statements\n",
    "'''\n",
    "def gen_feat_fls_sentiment(docs):\n",
    "    doc_list = [doc_dict['item_1a'] + '.' + doc_dict['item_7'] for doc_dict in docs.values()]\n",
    "    fls_ptrn = r\"(\\baim\\b|\\banticipate\\b|\\bbelieve\\b|\\bcould\\b|\\bcontinue\\b|\\bestimate\\b|\\bexpansion\\b|\\bexpect\\b|\\bexpectation\\b|\\bexpected to be\\b|\\bfocus\\b|\\bforecast\\b|\\bgoal\\b|\\bgrow\\b|\\bguidance\\b|\\bintend\\b|\\binvest\\b|\\bis expected\\b|\\bmay\\b|\\bobjective\\b|\\bplan\\b|\\bpriority\\b|\\bproject\\b|\\bstrategy\\b|\\bto be\\b|\\bwe'll\\b|\\bwill\\b|\\bwould\\b)\"\n",
    "    sentiment = []\n",
    "    for doc in doc_list:\n",
    "        sen_list = [x for x in tokenize.sent_tokenize(doc) if len(x)>=30 and len(x)<=1000 and re.match(r'[a-z]', x) and re.search(fls_ptrn, x)]\n",
    "        if len(sen_list)==0:\n",
    "            sentiment.append(np.NaN)\n",
    "            continue\n",
    "        batch_size = 8\n",
    "        n_batch = math.ceil(len(sen_list)/batch_size)\n",
    "        sentiment_sum = 0\n",
    "        for i in range(n_batch):\n",
    "            inputs = fb_tokenizer(sen_list[batch_size*i:batch_size*(i+1)], padding=True, truncation=True, return_tensors='pt').to(\"cuda:0\")\n",
    "            with torch.no_grad():\n",
    "                outputs = fb_model(**inputs)\n",
    "            sentiment_sum += float(torch.nn.functional.softmax(outputs.logits, dim=-1)[:,0].sum())\n",
    "            torch.cuda.empty_cache()\n",
    "        sentiment.append(sentiment_sum / len(sen_list))\n",
    "    feat = pd.Series(sentiment).ffill().diff()\n",
    "    return feat.rename('feat_fls_sentiment')\n",
    "\n",
    "\n",
    "'''\n",
    "Word2Vec\n",
    "'''\n",
    "def gen_feat_word2vec(docs):\n",
    "    doc_list = [doc_dict['full'] for doc_dict in docs.values()]\n",
    "    tf_vectors = global_tfidf_1g.transform(doc_list)[:,tfidf_1g_wv_idx]\n",
    "    tf_vectors = tf_vectors / tf_vectors.sum(axis=1)\n",
    "    avg_vecs = []\n",
    "    for i in range(len(doc_list)):\n",
    "        vec_sum = np.zeros(300)\n",
    "        for j in range(len(tfidf_1g_wv_idx)):\n",
    "            vec_sum += tf_vectors[i,j] * wv[tfidf_1g_wv_word[j]]\n",
    "        avg_vecs.append(vec_sum)\n",
    "    avg_vecs = np.concatenate(avg_vecs).reshape(len(doc_list), 300)\n",
    "    feat = pd.Series([cosine_similarity(tf_vectors[i-1:i+1,:])[0][1] if i > 0 else np.NaN for i in range(len(doc_list))])\n",
    "    return feat.rename('feat_word2vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a85bb6",
   "metadata": {
    "_cell_guid": "8ece8156-9ab4-4939-b442-802a81b83fd7",
    "_uuid": "fe6b112c-e9b5-4e4c-b177-f1ab4347a8fd",
    "papermill": {
     "duration": 0.041341,
     "end_time": "2022-05-13T12:23:56.807980",
     "exception": false,
     "start_time": "2022-05-13T12:23:56.766639",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Run All per CIK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f30e944a",
   "metadata": {
    "_cell_guid": "a4f7e3d9-cd05-4800-aa8a-c41a5ed12f60",
    "_uuid": "953c43db-798b-4f66-9063-6d74706d5465",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-05-13T12:23:56.895192Z",
     "iopub.status.busy": "2022-05-13T12:23:56.894499Z",
     "iopub.status.idle": "2022-05-13T12:23:56.907662Z",
     "shell.execute_reply": "2022-05-13T12:23:56.908152Z",
     "shell.execute_reply.started": "2022-05-06T04:16:00.485351Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.058396,
     "end_time": "2022-05-13T12:23:56.908345",
     "exception": false,
     "start_time": "2022-05-13T12:23:56.849949",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gen_signal(cik):\n",
    "    log(f'Started signal generation for CIK {cik}')\n",
    "    df = master_idx.loc[lambda x: x.cik==cik].sort_values('filing_date').reset_index(drop=True)\n",
    "    docs = {}\n",
    "    for i in range(len(df)):\n",
    "        \n",
    "        # load 10-K text from EDGAR html url\n",
    "        url = df.iloc[i]['url_10k']\n",
    "        doc_id = df.iloc[i]['doc_id']\n",
    "        txt = requests.get(url, headers={\"user-agent\": f\"chan_tai_man_{int(float(np.random.rand(1)) * 1e7)}@gmail.com\"}).text\n",
    "\n",
    "        # clean doc, extract items\n",
    "        txt = soup = BeautifulSoup(txt, 'lxml').get_text('|', strip=True)\n",
    "        txt = clean_doc1(txt)\n",
    "        item_pos = find_item_pos(txt)\n",
    "        doc_dict = {}\n",
    "        doc_dict['full'] = txt[item_pos.iloc[0]['item_1_pos_start'] :]\n",
    "        for item in item_ptrn1:\n",
    "            doc_dict[item] = txt[item_pos.iloc[0][f'{item}_pos_start'] : item_pos.iloc[0][f'{item}_pos_end']]\n",
    "        for x in doc_dict:\n",
    "            doc_dict[x] = clean_doc2(doc_dict[x])\n",
    "        docs[doc_id] = doc_dict\n",
    "        \n",
    "    # generate signal\n",
    "    feat_vecs = [pd.Series(list(docs.keys())).rename('doc_id')]\n",
    "    if mode in ['full','cpu']:\n",
    "        feat_vecs += [gen_feat_ch_full_len(docs),\n",
    "                        gen_feat_ch_item_1a_len(docs),\n",
    "                        gen_feat_ch_item_1b_len(docs),\n",
    "                        gen_feat_ch_item_3_len(docs),\n",
    "                        gen_feat_full_cos_1gram(docs),\n",
    "                        gen_feat_full_cos_2gram(docs),\n",
    "                        gen_feat_full_jac_1gram(docs),\n",
    "                        gen_feat_full_jac_2gram(docs),\n",
    "                        gen_feat_lm_postive(docs),\n",
    "                        gen_feat_lm_uncertainty(docs),\n",
    "                        gen_feat_lm_litigious(docs),\n",
    "                        gen_feat_word2vec(docs)]\n",
    "    if mode in ['full','gpu']:\n",
    "        feat_vecs += [gen_feat_sen_enc(docs),\n",
    "                        gen_feat_item_sentiment(docs),\n",
    "                        gen_feat_fls_sentiment(docs)]\n",
    "    feats = pd.concat(feat_vecs, axis=1)\n",
    "    log(f'Completed signal generation for CIK {cik}')\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0d4ce300",
   "metadata": {
    "_cell_guid": "5c32856c-1bab-4f9d-8a46-4c9116fcd439",
    "_uuid": "04b60d57-1f90-4eff-9565-ce36e380f40c",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-05-13T12:23:56.996524Z",
     "iopub.status.busy": "2022-05-13T12:23:56.994287Z",
     "iopub.status.idle": "2022-05-13T12:23:56.998274Z",
     "shell.execute_reply": "2022-05-13T12:23:56.998919Z",
     "shell.execute_reply.started": "2022-05-06T04:16:00.50312Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.048973,
     "end_time": "2022-05-13T12:23:56.999095",
     "exception": false,
     "start_time": "2022-05-13T12:23:56.950122",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # generate signal per CIK\n",
    "# feats = Parallel(n_jobs=-1)(delayed(gen_signal)(cik) for cik in master_idx.cik.unique())\n",
    "# feats = pd.concat(feats).sort_values('doc_id').reset_index(drop=True)\n",
    "\n",
    "# # map back to stock\n",
    "# df = master_idx[['doc_id','cik','entity','filing_date']].drop_duplicates()\n",
    "# feats = feats.merge(df, how='inner', on='doc_id')\n",
    "# feats = feats.merge(cik_map, how='inner', on='cik')\n",
    "# cols = [c for c in feats if c[:5]=='feat_']\n",
    "# feats = feats[[c for c in feats if c not in cols] + cols]\n",
    "# display(feats.loc[lambda x: x.isnull().sum(axis=1) > 0].groupby('cik')['doc_id'].count().loc[lambda x: x>1])\n",
    "# display(feats.head())\n",
    "\n",
    "# # export\n",
    "# feats.to_csv('feats.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d9ed98ae",
   "metadata": {
    "_cell_guid": "60e6ab58-a504-4b16-9e6c-a5cb6c1307c7",
    "_uuid": "f8f6d60e-7f9b-4d42-99ff-27cf8e31e091",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-05-13T12:23:57.086176Z",
     "iopub.status.busy": "2022-05-13T12:23:57.085502Z",
     "iopub.status.idle": "2022-05-13T12:23:57.089459Z",
     "shell.execute_reply": "2022-05-13T12:23:57.090013Z",
     "shell.execute_reply.started": "2022-05-06T04:16:00.518867Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.048688,
     "end_time": "2022-05-13T12:23:57.090196",
     "exception": false,
     "start_time": "2022-05-13T12:23:57.041508",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # show sample item extraction\n",
    "# df = master_idx.sample(10).sort_values('filing_date').reset_index(drop=True)\n",
    "# # df = master_idx.sort_values('filing_date').reset_index(drop=True)\n",
    "\n",
    "# for i in range(len(df)):\n",
    "\n",
    "#     print(df.iloc[i]['cik'])\n",
    "#     print(df.iloc[i]['doc_id'])\n",
    "#     print(df.iloc[i]['url_10k'])\n",
    "    \n",
    "#     # load 10-K text from EDGAR html url\n",
    "#     url = df.iloc[i]['url_10k']\n",
    "#     doc_id = df.iloc[i]['doc_id']\n",
    "#     txt = requests.get(url, headers={\"user-agent\": f\"chan_tai_man_{int(float(np.random.rand(1)) * 1e7)}@gmail.com\"}).text\n",
    "\n",
    "#     # clean doc, extract items\n",
    "#     txt = soup = BeautifulSoup(txt, 'lxml').get_text('|', strip=True)\n",
    "#     txt = clean_doc1(txt)\n",
    "#     item_pos = find_item_pos(txt, log_mode=False)\n",
    "#     doc_dict = {}\n",
    "#     doc_dict['full'] = txt[item_pos.iloc[0]['item_1_pos_start'] :]\n",
    "#     for item in item_ptrn1:\n",
    "#         doc_dict[item] = txt[item_pos.iloc[0][f'{item}_pos_start'] : item_pos.iloc[0][f'{item}_pos_end']]\n",
    "#     for x in doc_dict:\n",
    "#         doc_dict[x] = clean_doc2(doc_dict[x])\n",
    "#     show_item(doc_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60c76d4",
   "metadata": {
    "_cell_guid": "b8519bc9-11d2-41fc-8835-2f0f5fd74c43",
    "_uuid": "7db204c8-c80e-41bb-92d4-0707e1055a54",
    "papermill": {
     "duration": 0.042869,
     "end_time": "2022-05-13T12:23:57.175948",
     "exception": false,
     "start_time": "2022-05-13T12:23:57.133079",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Preprocessing Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "72c09f32",
   "metadata": {
    "_cell_guid": "5fa4473b-df08-4c36-9bcb-41f2a767eb9f",
    "_uuid": "d3afb611-621c-4c60-a47b-d1481447a721",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-05-13T12:23:57.265948Z",
     "iopub.status.busy": "2022-05-13T12:23:57.265155Z",
     "iopub.status.idle": "2022-05-13T13:50:02.870725Z",
     "shell.execute_reply": "2022-05-13T13:50:02.870148Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 5165.653299,
     "end_time": "2022-05-13T13:50:02.870904",
     "exception": false,
     "start_time": "2022-05-13T12:23:57.217605",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-05-13 20:24:00] Completed downloading doc 0\n",
      "[2022-05-13 20:24:04] Completed downloading doc 1\n",
      "[2022-05-13 20:24:08] Completed downloading doc 2\n",
      "[2022-05-13 20:24:14] Completed downloading doc 3\n",
      "[2022-05-13 20:24:16] Completed downloading doc 4\n",
      "[2022-05-13 20:24:21] Completed downloading doc 5\n",
      "[2022-05-13 20:24:24] Completed downloading doc 6\n",
      "[2022-05-13 20:24:31] Completed downloading doc 7\n",
      "[2022-05-13 20:24:37] Completed downloading doc 8\n",
      "[2022-05-13 20:24:52] Completed downloading doc 9\n",
      "[2022-05-13 20:24:58] Completed downloading doc 10\n",
      "[2022-05-13 20:25:03] Completed downloading doc 11\n",
      "[2022-05-13 20:25:07] Completed downloading doc 12\n",
      "[2022-05-13 20:25:10] Completed downloading doc 13\n",
      "[2022-05-13 20:25:14] Completed downloading doc 14\n",
      "[2022-05-13 20:25:19] Completed downloading doc 15\n",
      "[2022-05-13 20:25:22] Completed downloading doc 16\n",
      "[2022-05-13 20:25:23] Completed downloading doc 17\n",
      "[2022-05-13 20:25:27] Completed downloading doc 18\n",
      "[2022-05-13 20:25:33] Completed downloading doc 19\n",
      "[2022-05-13 20:25:35] Completed downloading doc 20\n",
      "[2022-05-13 20:25:39] Completed downloading doc 21\n",
      "[2022-05-13 20:25:42] Completed downloading doc 22\n",
      "[2022-05-13 20:25:45] Completed downloading doc 23\n",
      "[2022-05-13 20:25:48] Completed downloading doc 24\n",
      "[2022-05-13 20:25:51] Completed downloading doc 25\n",
      "[2022-05-13 20:25:56] Completed downloading doc 26\n",
      "[2022-05-13 20:25:59] Completed downloading doc 27\n",
      "[2022-05-13 20:26:02] Completed downloading doc 28\n",
      "[2022-05-13 20:26:06] Completed downloading doc 29\n",
      "[2022-05-13 20:26:12] Completed downloading doc 30\n",
      "[2022-05-13 20:26:16] Completed downloading doc 31\n",
      "[2022-05-13 20:26:27] Completed downloading doc 32\n",
      "[2022-05-13 20:26:34] Completed downloading doc 33\n",
      "[2022-05-13 20:26:38] Completed downloading doc 34\n",
      "[2022-05-13 20:26:40] Completed downloading doc 35\n",
      "[2022-05-13 20:26:45] Completed downloading doc 36\n",
      "[2022-05-13 20:26:48] Completed downloading doc 37\n",
      "[2022-05-13 20:26:52] Completed downloading doc 38\n",
      "[2022-05-13 20:26:56] Completed downloading doc 39\n",
      "[2022-05-13 20:27:02] Completed downloading doc 40\n",
      "[2022-05-13 20:27:07] Completed downloading doc 41\n",
      "[2022-05-13 20:27:11] Completed downloading doc 42\n",
      "[2022-05-13 20:27:14] Completed downloading doc 43\n",
      "[2022-05-13 20:27:17] Completed downloading doc 44\n",
      "[2022-05-13 20:27:22] Completed downloading doc 45\n",
      "[2022-05-13 20:27:26] Completed downloading doc 46\n",
      "[2022-05-13 20:27:29] Completed downloading doc 47\n",
      "[2022-05-13 20:27:33] Completed downloading doc 48\n",
      "[2022-05-13 20:27:36] Completed downloading doc 49\n",
      "[2022-05-13 20:27:39] Completed downloading doc 50\n",
      "[2022-05-13 20:27:42] Completed downloading doc 51\n",
      "[2022-05-13 20:27:49] Completed downloading doc 52\n",
      "[2022-05-13 20:27:53] Completed downloading doc 53\n",
      "[2022-05-13 20:27:55] Completed downloading doc 54\n",
      "[2022-05-13 20:27:59] Completed downloading doc 55\n",
      "[2022-05-13 20:28:04] Completed downloading doc 56\n",
      "[2022-05-13 20:28:10] Completed downloading doc 57\n",
      "[2022-05-13 20:28:14] Completed downloading doc 58\n",
      "[2022-05-13 20:28:18] Completed downloading doc 59\n",
      "[2022-05-13 20:28:24] Completed downloading doc 60\n",
      "[2022-05-13 20:28:25] Completed downloading doc 61\n",
      "[2022-05-13 20:28:33] Completed downloading doc 62\n",
      "[2022-05-13 20:28:35] Completed downloading doc 63\n",
      "[2022-05-13 20:28:39] Completed downloading doc 64\n",
      "[2022-05-13 20:28:43] Completed downloading doc 65\n",
      "[2022-05-13 20:28:49] Completed downloading doc 66\n",
      "[2022-05-13 20:28:53] Completed downloading doc 67\n",
      "[2022-05-13 19:48:30] [1200] https://www.sec.gov/Archives/edgar/data/72903/000114036111012444/form10-k.htm\n",
      "[2022-05-13 19:51:45] [3000] https://www.sec.gov/Archives/edgar/data/875320/000087532015000012/a201410k-main.htm\n",
      "[2022-05-13 19:52:27] [3400] https://www.sec.gov/Archives/edgar/data/921847/000119312512084872/d264433d10k.htm\n",
      "[2022-05-13 19:54:15] [4400] https://www.sec.gov/Archives/edgar/data/1121788/000161577418001344/s109029_10k.htm\n",
      "[2022-05-13 19:55:21] [5000] https://www.sec.gov/Archives/edgar/data/1418135/000095013409006140/d66682e10vk.htm\n",
      "[2022-05-13 19:55:43] [5200] https://www.sec.gov/Archives/edgar/data/1555280/000155528018000053/zoetis-20171231x10kye.htm\n",
      "[2022-05-13 19:57:40] [1000] https://www.sec.gov/Archives/edgar/data/20286/000002028613000013/cinf-2013630x10q.htm\n",
      "[2022-05-13 19:59:29] [2000] https://www.sec.gov/Archives/edgar/data/39899/000119312511208158/d10q.htm\n",
      "[2022-05-13 20:03:07] [4000] https://www.sec.gov/Archives/edgar/data/85408/000008540813000015/rdc-2013093010q.htm\n",
      "[2022-05-13 20:03:49] [4400] https://www.sec.gov/Archives/edgar/data/93410/000009341014000042/cvx-06302014x10qdoc.htm\n",
      "[2022-05-13 20:04:56] [5000] https://www.sec.gov/Archives/edgar/data/106640/000010664016000111/whr0630201610-q.htm\n",
      "[2022-05-13 20:06:46] [6000] https://www.sec.gov/Archives/edgar/data/356028/000035602813000153/ca-20130630x10q.htm\n",
      "[2022-05-13 20:07:07] [6200] https://www.sec.gov/Archives/edgar/data/713676/000119312516577177/d170252d10q.htm\n",
      "[2022-05-13 20:07:51] [6600] https://www.sec.gov/Archives/edgar/data/740260/000074026015000131/vtr-2015630x10q.htm\n",
      "[2022-05-13 20:08:34] [7000] https://www.sec.gov/Archives/edgar/data/766704/000076670412000020/10-Q.htm\n",
      "[2022-05-13 20:08:55] [7200] https://www.sec.gov/Archives/edgar/data/788784/000078878417000019/pseg-6302017xq2.htm\n",
      "[2022-05-13 20:09:16] [7400] https://www.sec.gov/Archives/edgar/data/794367/000079436715000123/m-0502201510q.htm\n",
      "[2022-05-13 20:11:47] [8800] https://www.sec.gov/Archives/edgar/data/875045/000095013509002908/b75004bie10vq.htm\n",
      "[2022-05-13 20:12:30] [9200] https://www.sec.gov/Archives/edgar/data/890801/000095013409009941/d66889e10vq.htm\n",
      "[2022-05-13 20:16:05] [11200] https://www.sec.gov/Archives/edgar/data/1032208/000008652116000100/sre10q03312016.htm\n",
      "[2022-05-13 20:16:48] [11600] https://www.sec.gov/Archives/edgar/data/1045810/000104581011000056/nvda2011q310q.htm\n",
      "[2022-05-13 20:19:43] [13200] https://www.sec.gov/Archives/edgar/data/1137774/000119312508162674/d10q.htm\n",
      "[2022-05-13 19:46:17] [0] https://www.sec.gov/Archives/edgar/data/1800/000104746908001480/a2182445z10-k.htm\n",
      "[2022-05-13 19:46:39] [200] https://www.sec.gov/Archives/edgar/data/9892/000119312511044634/d10k.htm\n",
      "[2022-05-13 19:48:09] [1000] https://www.sec.gov/Archives/edgar/data/60086/000006008608000018/form10k.htm\n",
      "[2022-05-13 19:49:35] [1800] https://www.sec.gov/Archives/edgar/data/277135/000027713515000005/gww20141231-10k.htm\n",
      "[2022-05-13 19:51:23] [2800] https://www.sec.gov/Archives/edgar/data/831001/000120677411000316/citigroup_10k.htm\n",
      "[2022-05-13 19:52:06] [3200] https://www.sec.gov/Archives/edgar/data/898293/000119312514377410/d768514d10k.htm\n",
      "[2022-05-13 19:53:32] [4000] https://www.sec.gov/Archives/edgar/data/1048911/000119312514267851/d752614d10k.htm\n",
      "[2022-05-13 19:53:53] [4200] https://www.sec.gov/Archives/edgar/data/1075531/000107553115000007/pcln-20141231_10k.htm\n",
      "[2022-05-13 19:55:52] [0] https://www.sec.gov/Archives/edgar/data/1800/000110465907058777/a07-18715_110q.htm\n",
      "[2022-05-13 19:56:13] [200] https://www.sec.gov/Archives/edgar/data/4962/000119312512200270/d339106d10q.htm\n",
      "[2022-05-13 19:56:35] [400] https://www.sec.gov/Archives/edgar/data/7084/000000708410000006/adm10qfy10q2.htm\n",
      "[2022-05-13 19:58:24] [1400] https://www.sec.gov/Archives/edgar/data/29669/000119312508168396/d10q.htm\n",
      "[2022-05-13 19:58:46] [1600] https://www.sec.gov/Archives/edgar/data/32604/000003260414000010/emr-12312013x10q.htm\n",
      "[2022-05-13 19:59:07] [1800] https://www.sec.gov/Archives/edgar/data/36104/000119312514400765/d772448d10q.htm\n",
      "[2022-05-13 20:01:17] [3000] https://www.sec.gov/Archives/edgar/data/62709/000119312508107708/d10q.htm\n",
      "[2022-05-13 20:05:18] [5200] https://www.sec.gov/Archives/edgar/data/203527/000119312509171667/d10q.htm\n",
      "[2022-05-13 20:06:02] [5600] https://www.sec.gov/Archives/edgar/data/316206/000095012311069587/p18899e10vq.htm\n",
      "[2022-05-13 20:08:12] [6800] https://www.sec.gov/Archives/edgar/data/754737/000075473712000037/a2012630-10q.htm\n",
      "[2022-05-13 20:09:59] [7800] https://www.sec.gov/Archives/edgar/data/814453/000081445316000273/nwl-10qx2016xq3.htm\n",
      "[2022-05-13 20:11:05] [8400] https://www.sec.gov/Archives/edgar/data/849399/000095013407022679/f34786e10vq.htm\n",
      "[2022-05-13 20:13:34] [9800] https://www.sec.gov/Archives/edgar/data/915912/000110465913040124/a13-8204_110q.htm\n",
      "[2022-05-13 20:14:39] [10400] https://www.sec.gov/Archives/edgar/data/945764/000094576412000067/form10q.htm\n",
      "[2022-05-13 20:15:23] [10800] https://www.sec.gov/Archives/edgar/data/1013871/000095012309007582/y76733e10vq.htm\n",
      "[2022-05-13 20:16:27] [11400] https://www.sec.gov/Archives/edgar/data/1038357/000103835714000036/pxd-20140331.htm\n",
      "[2022-05-13 20:17:10] [11800] https://www.sec.gov/Archives/edgar/data/1050915/000119312515284664/d928298d10q.htm\n",
      "[2022-05-13 20:20:29] [13600] https://www.sec.gov/Archives/edgar/data/1166126/000116612609000048/jcpenney10q2ndquater2009.htm\n",
      "[2022-05-13 20:20:51] [13800] https://www.sec.gov/Archives/edgar/data/1274057/000127405713000025/hsp-2013930x10q.htm\n",
      "[2022-05-13 20:21:13] [14000] https://www.sec.gov/Archives/edgar/data/1289490/000162828017010869/exr-09302017x10q.htm\n",
      "[2022-05-13 20:21:37] [14200] https://www.sec.gov/Archives/edgar/data/1336920/000119312509249604/d10q.htm\n",
      "[2022-05-13 20:22:00] [14400] https://www.sec.gov/Archives/edgar/data/1368007/000119312509107398/d10q.htm\n",
      "[2022-05-13 20:22:21] [14600] https://www.sec.gov/Archives/edgar/data/1396009/000095012311072749/g27589e10vq.htm\n",
      "[2022-05-13 20:23:25] [15200] https://www.sec.gov/Archives/edgar/data/1534701/000153470113000022/psx-2013930x10q.htm\n",
      "[2022-05-13 19:47:47] [800] https://www.sec.gov/Archives/edgar/data/47111/000119312512067143/d251393d10k.htm\n",
      "[2022-05-13 19:48:52] [1400] https://www.sec.gov/Archives/edgar/data/87347/000156459015000337/slb-10k_20141231.htm\n",
      "[2022-05-13 19:49:13] [1600] https://www.sec.gov/Archives/edgar/data/100493/000010049313000079/tsn2013q410k.htm\n",
      "[2022-05-13 19:49:57] [2000] https://www.sec.gov/Archives/edgar/data/350698/000035069817000028/an10k2016.htm\n",
      "[2022-05-13 19:52:49] [3600] https://www.sec.gov/Archives/edgar/data/1002047/000119312514239169/d691214d10k.htm\n",
      "[2022-05-13 19:58:02] [1200] https://www.sec.gov/Archives/edgar/data/24545/000104746908011752/a2188761z10-q.htm\n",
      "[2022-05-13 20:00:35] [2600] https://www.sec.gov/Archives/edgar/data/51253/000095012309029974/y02036e10vq.htm\n",
      "[2022-05-13 20:00:56] [2800] https://www.sec.gov/Archives/edgar/data/55785/000005578516000152/kmb_10qx2016xq1.htm\n",
      "[2022-05-13 20:01:39] [3200] https://www.sec.gov/Archives/edgar/data/64803/000119312510246290/d10q.htm\n",
      "[2022-05-13 20:02:45] [3800] https://www.sec.gov/Archives/edgar/data/78003/000007800315000026/pfe-3292015x10q.htm\n",
      "[2022-05-13 20:11:26] [8600] https://www.sec.gov/Archives/edgar/data/864328/000119312508109699/d10q.htm\n",
      "[2022-05-13 20:12:09] [9000] https://www.sec.gov/Archives/edgar/data/882835/000088283513000022/q1_2013-10q.htm\n",
      "[2022-05-13 20:12:51] [9400] https://www.sec.gov/Archives/edgar/data/898293/000119312508078130/d10q.htm\n",
      "[2022-05-13 20:14:17] [10200] https://www.sec.gov/Archives/edgar/data/931336/000119312512340986/d363454d10q.htm\n",
      "[2022-05-13 20:17:54] [12200] https://www.sec.gov/Archives/edgar/data/1065696/000106569617000057/lkq-2017093010xq.htm\n",
      "[2022-05-13 20:18:38] [12600] https://www.sec.gov/Archives/edgar/data/1099800/000104746914006621/a2220912z10-q.htm\n",
      "[2022-05-13 20:19:00] [12800] https://www.sec.gov/Archives/edgar/data/1111711/000119312511285590/d230358d10q.htm\n",
      "[2022-05-13 20:20:07] [13400] https://www.sec.gov/Archives/edgar/data/1144215/000114421515000077/ayi-20150531x10q.htm\n",
      "[2022-05-13 20:22:42] [14800] https://www.sec.gov/Archives/edgar/data/1437107/000143710713000042/disca-201393010q.htm\n",
      "[2022-05-13 20:23:04] [15000] https://www.sec.gov/Archives/edgar/data/1478242/000156459016017395/q-10q_20160331.htm\n",
      "[2022-05-13 20:23:48] [15400] https://www.sec.gov/Archives/edgar/data/1613103/000161310315000008/mdt-plcx2015q3x10q.htm\n",
      "[2022-05-13 19:47:02] [400] https://www.sec.gov/Archives/edgar/data/23217/000144530513001653/cag-2013x10k.htm\n",
      "[2022-05-13 19:47:25] [600] https://www.sec.gov/Archives/edgar/data/35527/000119312513071733/d479526d10k.htm\n",
      "[2022-05-13 19:50:18] [2200] https://www.sec.gov/Archives/edgar/data/728535/000143774914002605/jbht20131231_10k.htm\n",
      "[2022-05-13 19:50:40] [2400] https://www.sec.gov/Archives/edgar/data/769397/000076939716000067/adsk-0131201610xk.htm\n",
      "[2022-05-13 19:51:01] [2600] https://www.sec.gov/Archives/edgar/data/804753/000080475313000004/a201210-k.htm\n",
      "[2022-05-13 19:53:10] [3800] https://www.sec.gov/Archives/edgar/data/1032208/000008652110000019/sre_200910k.htm\n",
      "[2022-05-13 19:54:38] [4600] https://www.sec.gov/Archives/edgar/data/1163165/000119312518049729/d534096d10k.htm\n",
      "[2022-05-13 19:54:59] [4800] https://www.sec.gov/Archives/edgar/data/1326160/000119312510043083/d10k.htm\n",
      "[2022-05-13 19:56:57] [600] https://www.sec.gov/Archives/edgar/data/9892/000119312516747585/d253269d10q.htm\n",
      "[2022-05-13 19:57:18] [800] https://www.sec.gov/Archives/edgar/data/14693/000001469311000098/form10-q.htm\n",
      "[2022-05-13 19:59:51] [2200] https://www.sec.gov/Archives/edgar/data/42582/000095012309027487/l37107e10vq.htm\n",
      "[2022-05-13 20:00:14] [2400] https://www.sec.gov/Archives/edgar/data/47217/000004721717000029/hp-73117x10q.htm\n",
      "[2022-05-13 20:02:01] [3400] https://www.sec.gov/Archives/edgar/data/70858/000007085816000222/bac-930201610xq.htm\n",
      "[2022-05-13 20:02:23] [3600] https://www.sec.gov/Archives/edgar/data/73124/000007312415000217/a2015q3form10-q.htm\n",
      "[2022-05-13 20:03:28] [4200] https://www.sec.gov/Archives/edgar/data/91419/000119312512107187/d286862d10q.htm\n",
      "[2022-05-13 20:04:11] [4600] https://www.sec.gov/Archives/edgar/data/97476/000009747612000027/txn-2012331x10q.htm\n",
      "[2022-05-13 20:04:33] [4800] https://www.sec.gov/Archives/edgar/data/101829/000119312510165467/d10q.htm\n",
      "[2022-05-13 20:05:40] [5400] https://www.sec.gov/Archives/edgar/data/313616/000119312507157383/d10q.htm\n",
      "[2022-05-13 20:06:24] [5800] https://www.sec.gov/Archives/edgar/data/320335/000119312510251848/d10q.htm\n",
      "[2022-05-13 20:07:29] [6400] https://www.sec.gov/Archives/edgar/data/723254/000110465912001090/a11-29943_110q.htm\n",
      "[2022-05-13 20:09:37] [7600] https://www.sec.gov/Archives/edgar/data/804212/000080421213000004/arg-12311210xq.htm\n",
      "[2022-05-13 20:10:21] [8000] https://www.sec.gov/Archives/edgar/data/820027/000082002717000058/amp09302017.htm\n",
      "[2022-05-13 20:10:43] [8200] https://www.sec.gov/Archives/edgar/data/827099/000103883810000154/q033110.htm\n",
      "[2022-05-13 20:13:13] [9600] https://www.sec.gov/Archives/edgar/data/909954/000110465914006821/a13-26524_110q.htm\n",
      "[2022-05-13 20:13:56] [10000] https://www.sec.gov/Archives/edgar/data/921847/000119312513434652/d602669d10q.htm\n",
      "[2022-05-13 20:15:01] [10600] https://www.sec.gov/Archives/edgar/data/1002910/000100291007000128/ameren10q06302007.htm\n",
      "[2022-05-13 20:15:44] [11000] https://www.sec.gov/Archives/edgar/data/1020569/000104746913005389/a2214768z10-q.htm\n",
      "[2022-05-13 20:17:32] [12000] https://www.sec.gov/Archives/edgar/data/1058290/000105829016000074/ctsh2016630-10q.htm\n",
      "[2022-05-13 20:18:16] [12400] https://www.sec.gov/Archives/edgar/data/1087423/000119312508149201/d10q.htm\n",
      "[2022-05-13 20:19:21] [13000] https://www.sec.gov/Archives/edgar/data/1126328/000110465914054822/a14-14032_110q.htm\n",
      "[2022-05-13 20:28:56] Completed downloading doc 68\n",
      "[2022-05-13 20:29:00] Completed downloading doc 69\n",
      "[2022-05-13 20:29:02] Completed downloading doc 70\n",
      "[2022-05-13 20:29:06] Completed downloading doc 71\n",
      "[2022-05-13 20:29:11] Completed downloading doc 72\n",
      "[2022-05-13 20:29:15] Completed downloading doc 73\n",
      "[2022-05-13 20:29:17] Completed downloading doc 74\n",
      "[2022-05-13 20:29:22] Completed downloading doc 75\n",
      "[2022-05-13 20:29:26] Completed downloading doc 76\n",
      "[2022-05-13 20:29:29] Completed downloading doc 77\n",
      "[2022-05-13 20:29:32] Completed downloading doc 78\n",
      "[2022-05-13 20:29:35] Completed downloading doc 79\n",
      "[2022-05-13 20:29:41] Completed downloading doc 80\n",
      "[2022-05-13 20:29:44] Completed downloading doc 81\n",
      "[2022-05-13 20:29:48] Completed downloading doc 82\n",
      "[2022-05-13 20:29:50] Completed downloading doc 83\n",
      "[2022-05-13 20:29:54] Completed downloading doc 84\n",
      "[2022-05-13 20:30:01] Completed downloading doc 85\n",
      "[2022-05-13 20:30:04] Completed downloading doc 86\n",
      "[2022-05-13 20:30:07] Completed downloading doc 87\n",
      "[2022-05-13 20:30:08] Completed downloading doc 88\n",
      "[2022-05-13 20:30:12] Completed downloading doc 89\n",
      "[2022-05-13 20:30:16] Completed downloading doc 90\n",
      "[2022-05-13 20:30:19] Completed downloading doc 91\n",
      "[2022-05-13 20:30:23] Completed downloading doc 92\n",
      "[2022-05-13 20:30:27] Completed downloading doc 93\n",
      "[2022-05-13 20:30:31] Completed downloading doc 94\n",
      "[2022-05-13 20:30:34] Completed downloading doc 95\n",
      "[2022-05-13 20:30:38] Completed downloading doc 96\n",
      "[2022-05-13 20:30:42] Completed downloading doc 97\n",
      "[2022-05-13 20:30:46] Completed downloading doc 98\n",
      "[2022-05-13 20:30:55] Completed downloading doc 99\n",
      "[2022-05-13 20:31:01] Completed downloading doc 100\n",
      "[2022-05-13 20:31:04] Completed downloading doc 101\n",
      "[2022-05-13 20:31:07] Completed downloading doc 102\n",
      "[2022-05-13 20:31:10] Completed downloading doc 103\n",
      "[2022-05-13 20:31:14] Completed downloading doc 104\n",
      "[2022-05-13 20:31:17] Completed downloading doc 105\n",
      "[2022-05-13 20:31:20] Completed downloading doc 106\n",
      "[2022-05-13 20:31:24] Completed downloading doc 107\n",
      "[2022-05-13 20:31:26] Completed downloading doc 108\n",
      "[2022-05-13 20:31:39] Completed downloading doc 109\n",
      "[2022-05-13 20:31:42] Completed downloading doc 110\n",
      "[2022-05-13 20:31:49] Completed downloading doc 111\n",
      "[2022-05-13 20:31:52] Completed downloading doc 112\n",
      "[2022-05-13 20:31:55] Completed downloading doc 113\n",
      "[2022-05-13 20:32:01] Completed downloading doc 114\n",
      "[2022-05-13 20:32:11] Completed downloading doc 115\n",
      "[2022-05-13 20:32:16] Completed downloading doc 116\n",
      "[2022-05-13 20:32:20] Completed downloading doc 117\n",
      "[2022-05-13 20:32:23] Completed downloading doc 118\n",
      "[2022-05-13 20:32:30] Completed downloading doc 119\n",
      "[2022-05-13 20:32:35] Completed downloading doc 120\n",
      "[2022-05-13 20:32:36] Completed downloading doc 121\n",
      "[2022-05-13 20:32:43] Completed downloading doc 122\n",
      "[2022-05-13 20:32:44] Completed downloading doc 123\n",
      "[2022-05-13 20:32:52] Completed downloading doc 124\n",
      "[2022-05-13 20:32:56] Completed downloading doc 125\n",
      "[2022-05-13 20:32:59] Completed downloading doc 126\n",
      "[2022-05-13 20:33:02] Completed downloading doc 127\n",
      "[2022-05-13 20:33:05] Completed downloading doc 128\n",
      "[2022-05-13 20:33:09] Completed downloading doc 129\n",
      "[2022-05-13 20:33:11] Completed downloading doc 130\n",
      "[2022-05-13 20:33:15] Completed downloading doc 131\n",
      "[2022-05-13 20:33:18] Completed downloading doc 132\n",
      "[2022-05-13 20:33:21] Completed downloading doc 133\n",
      "[2022-05-13 20:33:24] Completed downloading doc 134\n",
      "[2022-05-13 20:33:27] Completed downloading doc 135\n",
      "[2022-05-13 20:33:29] Completed downloading doc 136\n",
      "[2022-05-13 20:33:32] Completed downloading doc 137\n",
      "[2022-05-13 20:33:37] Completed downloading doc 138\n",
      "[2022-05-13 20:33:40] Completed downloading doc 139\n",
      "[2022-05-13 20:33:50] Completed downloading doc 140\n",
      "[2022-05-13 20:33:53] Completed downloading doc 141\n",
      "[2022-05-13 20:33:55] Completed downloading doc 142\n",
      "[2022-05-13 20:33:57] Completed downloading doc 143\n",
      "[2022-05-13 20:34:00] Completed downloading doc 144\n",
      "[2022-05-13 20:34:01] Completed downloading doc 145\n",
      "[2022-05-13 20:34:05] Completed downloading doc 146\n",
      "[2022-05-13 20:34:12] Completed downloading doc 147\n",
      "[2022-05-13 20:34:30] Completed downloading doc 148\n",
      "[2022-05-13 20:34:37] Completed downloading doc 149\n",
      "[2022-05-13 20:34:40] Completed downloading doc 150\n",
      "[2022-05-13 20:34:45] Completed downloading doc 151\n",
      "[2022-05-13 20:34:48] Completed downloading doc 152\n",
      "[2022-05-13 20:34:56] Completed downloading doc 153\n",
      "[2022-05-13 20:34:59] Completed downloading doc 154\n",
      "[2022-05-13 20:35:03] Completed downloading doc 155\n",
      "[2022-05-13 20:35:09] Completed downloading doc 156\n",
      "[2022-05-13 20:35:12] Completed downloading doc 157\n",
      "[2022-05-13 20:35:16] Completed downloading doc 158\n",
      "[2022-05-13 20:35:20] Completed downloading doc 159\n",
      "[2022-05-13 20:35:23] Completed downloading doc 160\n",
      "[2022-05-13 20:35:27] Completed downloading doc 161\n",
      "[2022-05-13 20:35:30] Completed downloading doc 162\n",
      "[2022-05-13 20:35:34] Completed downloading doc 163\n",
      "[2022-05-13 20:35:37] Completed downloading doc 164\n",
      "[2022-05-13 20:35:41] Completed downloading doc 165\n",
      "[2022-05-13 20:35:43] Completed downloading doc 166\n",
      "[2022-05-13 20:35:46] Completed downloading doc 167\n",
      "[2022-05-13 20:35:48] Completed downloading doc 168\n",
      "[2022-05-13 20:35:52] Completed downloading doc 169\n",
      "[2022-05-13 20:35:56] Completed downloading doc 170\n",
      "[2022-05-13 20:35:59] Completed downloading doc 171\n",
      "[2022-05-13 20:36:02] Completed downloading doc 172\n",
      "[2022-05-13 20:36:07] Completed downloading doc 173\n",
      "[2022-05-13 20:36:10] Completed downloading doc 174\n",
      "[2022-05-13 20:36:15] Completed downloading doc 175\n",
      "[2022-05-13 20:36:19] Completed downloading doc 176\n",
      "[2022-05-13 20:36:23] Completed downloading doc 177\n",
      "[2022-05-13 20:36:26] Completed downloading doc 178\n",
      "[2022-05-13 20:36:30] Completed downloading doc 179\n",
      "[2022-05-13 20:36:34] Completed downloading doc 180\n",
      "[2022-05-13 20:36:38] Completed downloading doc 181\n",
      "[2022-05-13 20:36:40] Completed downloading doc 182\n",
      "[2022-05-13 20:36:44] Completed downloading doc 183\n",
      "[2022-05-13 20:36:48] Completed downloading doc 184\n",
      "[2022-05-13 20:36:50] Completed downloading doc 185\n",
      "[2022-05-13 20:36:54] Completed downloading doc 186\n",
      "[2022-05-13 20:36:57] Completed downloading doc 187\n",
      "[2022-05-13 20:37:02] Completed downloading doc 188\n",
      "[2022-05-13 20:37:07] Completed downloading doc 189\n",
      "[2022-05-13 20:37:09] Completed downloading doc 190\n",
      "[2022-05-13 20:37:13] Completed downloading doc 191\n",
      "[2022-05-13 20:37:18] Completed downloading doc 192\n",
      "[2022-05-13 20:37:22] Completed downloading doc 193\n",
      "[2022-05-13 20:37:26] Completed downloading doc 194\n",
      "[2022-05-13 20:37:27] Completed downloading doc 195\n",
      "[2022-05-13 20:37:31] Completed downloading doc 196\n",
      "[2022-05-13 20:37:34] Completed downloading doc 197\n",
      "[2022-05-13 20:37:37] Completed downloading doc 198\n",
      "[2022-05-13 20:37:40] Completed downloading doc 199\n",
      "[2022-05-13 20:37:45] Completed downloading doc 200\n",
      "[2022-05-13 20:37:52] Completed downloading doc 201\n",
      "[2022-05-13 20:37:55] Completed downloading doc 202\n",
      "[2022-05-13 20:38:01] Completed downloading doc 203\n",
      "[2022-05-13 20:38:05] Completed downloading doc 204\n",
      "[2022-05-13 20:38:10] Completed downloading doc 205\n",
      "[2022-05-13 20:38:13] Completed downloading doc 206\n",
      "[2022-05-13 20:38:16] Completed downloading doc 207\n",
      "[2022-05-13 20:38:19] Completed downloading doc 208\n",
      "[2022-05-13 20:38:22] Completed downloading doc 209\n",
      "[2022-05-13 20:38:25] Completed downloading doc 210\n",
      "[2022-05-13 20:38:29] Completed downloading doc 211\n",
      "[2022-05-13 20:38:33] Completed downloading doc 212\n",
      "[2022-05-13 20:38:36] Completed downloading doc 213\n",
      "[2022-05-13 20:38:39] Completed downloading doc 214\n",
      "[2022-05-13 20:38:46] Completed downloading doc 215\n",
      "[2022-05-13 20:38:53] Completed downloading doc 216\n",
      "[2022-05-13 20:38:58] Completed downloading doc 217\n",
      "[2022-05-13 20:39:03] Completed downloading doc 218\n",
      "[2022-05-13 20:39:09] Completed downloading doc 219\n",
      "[2022-05-13 20:39:12] Completed downloading doc 220\n",
      "[2022-05-13 20:39:14] Completed downloading doc 221\n",
      "[2022-05-13 20:39:17] Completed downloading doc 222\n",
      "[2022-05-13 20:39:20] Completed downloading doc 223\n",
      "[2022-05-13 20:39:24] Completed downloading doc 224\n",
      "[2022-05-13 20:39:50] Completed downloading doc 225\n",
      "[2022-05-13 20:39:53] Completed downloading doc 226\n",
      "[2022-05-13 20:39:57] Completed downloading doc 227\n",
      "[2022-05-13 20:39:58] Completed downloading doc 228\n",
      "[2022-05-13 20:39:59] Completed downloading doc 229\n",
      "[2022-05-13 20:40:08] Completed downloading doc 230\n",
      "[2022-05-13 20:40:11] Completed downloading doc 231\n",
      "[2022-05-13 20:40:13] Completed downloading doc 232\n",
      "[2022-05-13 20:40:16] Completed downloading doc 233\n",
      "[2022-05-13 20:40:19] Completed downloading doc 234\n",
      "[2022-05-13 20:40:26] Completed downloading doc 235\n",
      "[2022-05-13 20:40:29] Completed downloading doc 236\n",
      "[2022-05-13 20:40:34] Completed downloading doc 237\n",
      "[2022-05-13 20:40:38] Completed downloading doc 238\n",
      "[2022-05-13 20:40:46] Completed downloading doc 239\n",
      "[2022-05-13 20:40:51] Completed downloading doc 240\n",
      "[2022-05-13 20:40:56] Completed downloading doc 241\n",
      "[2022-05-13 20:40:59] Completed downloading doc 242\n",
      "[2022-05-13 20:41:05] Completed downloading doc 243\n",
      "[2022-05-13 20:41:12] Completed downloading doc 244\n",
      "[2022-05-13 20:41:14] Completed downloading doc 245\n",
      "[2022-05-13 20:41:25] Completed downloading doc 246\n",
      "[2022-05-13 20:41:28] Completed downloading doc 247\n",
      "[2022-05-13 20:41:31] Completed downloading doc 248\n",
      "[2022-05-13 20:41:35] Completed downloading doc 249\n",
      "[2022-05-13 20:41:41] Completed downloading doc 250\n",
      "[2022-05-13 20:41:47] Completed downloading doc 251\n",
      "[2022-05-13 20:41:52] Completed downloading doc 252\n",
      "[2022-05-13 20:41:59] Completed downloading doc 253\n",
      "[2022-05-13 20:42:03] Completed downloading doc 254\n",
      "[2022-05-13 20:42:08] Completed downloading doc 255\n",
      "[2022-05-13 20:42:12] Completed downloading doc 256\n",
      "[2022-05-13 20:42:15] Completed downloading doc 257\n",
      "[2022-05-13 20:42:18] Completed downloading doc 258\n",
      "[2022-05-13 20:42:22] Completed downloading doc 259\n",
      "[2022-05-13 20:42:26] Completed downloading doc 260\n",
      "[2022-05-13 20:42:30] Completed downloading doc 261\n",
      "[2022-05-13 20:42:34] Completed downloading doc 262\n",
      "[2022-05-13 20:42:38] Completed downloading doc 263\n",
      "[2022-05-13 20:42:42] Completed downloading doc 264\n",
      "[2022-05-13 20:42:45] Completed downloading doc 265\n",
      "[2022-05-13 20:42:48] Completed downloading doc 266\n",
      "[2022-05-13 20:42:51] Completed downloading doc 267\n",
      "[2022-05-13 20:42:55] Completed downloading doc 268\n",
      "[2022-05-13 20:42:59] Completed downloading doc 269\n",
      "[2022-05-13 20:43:02] Completed downloading doc 270\n",
      "[2022-05-13 20:43:06] Completed downloading doc 271\n",
      "[2022-05-13 20:43:09] Completed downloading doc 272\n",
      "[2022-05-13 20:43:15] Completed downloading doc 273\n",
      "[2022-05-13 20:43:22] Completed downloading doc 274\n",
      "[2022-05-13 20:43:25] Completed downloading doc 275\n",
      "[2022-05-13 20:43:30] Completed downloading doc 276\n",
      "[2022-05-13 20:43:33] Completed downloading doc 277\n",
      "[2022-05-13 20:43:41] Completed downloading doc 278\n",
      "[2022-05-13 20:43:43] Completed downloading doc 279\n",
      "[2022-05-13 20:43:45] Completed downloading doc 280\n",
      "[2022-05-13 20:43:49] Completed downloading doc 281\n",
      "[2022-05-13 20:43:52] Completed downloading doc 282\n",
      "[2022-05-13 20:43:57] Completed downloading doc 283\n",
      "[2022-05-13 20:44:03] Completed downloading doc 284\n",
      "[2022-05-13 20:44:07] Completed downloading doc 285\n",
      "[2022-05-13 20:44:11] Completed downloading doc 286\n",
      "[2022-05-13 20:44:14] Completed downloading doc 287\n",
      "[2022-05-13 20:44:19] Completed downloading doc 288\n",
      "[2022-05-13 20:44:23] Completed downloading doc 289\n",
      "[2022-05-13 20:44:27] Completed downloading doc 290\n",
      "[2022-05-13 20:44:28] Completed downloading doc 291\n",
      "[2022-05-13 20:44:32] Completed downloading doc 292\n",
      "[2022-05-13 20:44:42] Completed downloading doc 293\n",
      "[2022-05-13 20:44:49] Completed downloading doc 294\n",
      "[2022-05-13 20:44:55] Completed downloading doc 295\n",
      "[2022-05-13 20:45:00] Completed downloading doc 296\n",
      "[2022-05-13 20:45:03] Completed downloading doc 297\n",
      "[2022-05-13 20:45:07] Completed downloading doc 298\n",
      "[2022-05-13 20:45:13] Completed downloading doc 299\n",
      "[2022-05-13 20:45:16] Completed downloading doc 300\n",
      "[2022-05-13 20:45:19] Completed downloading doc 301\n",
      "[2022-05-13 20:45:22] Completed downloading doc 302\n",
      "[2022-05-13 20:45:25] Completed downloading doc 303\n",
      "[2022-05-13 20:45:29] Completed downloading doc 304\n",
      "[2022-05-13 20:45:33] Completed downloading doc 305\n",
      "[2022-05-13 20:45:37] Completed downloading doc 306\n",
      "[2022-05-13 20:45:41] Completed downloading doc 307\n",
      "[2022-05-13 20:45:43] Completed downloading doc 308\n",
      "[2022-05-13 20:45:46] Completed downloading doc 309\n",
      "[2022-05-13 20:45:48] Completed downloading doc 310\n",
      "[2022-05-13 20:45:52] Completed downloading doc 311\n",
      "[2022-05-13 20:45:54] Completed downloading doc 312\n",
      "[2022-05-13 20:45:57] Completed downloading doc 313\n",
      "[2022-05-13 20:46:02] Completed downloading doc 314\n",
      "[2022-05-13 20:46:06] Completed downloading doc 315\n",
      "[2022-05-13 20:46:12] Completed downloading doc 316\n",
      "[2022-05-13 20:46:19] Completed downloading doc 317\n",
      "[2022-05-13 20:46:24] Completed downloading doc 318\n",
      "[2022-05-13 20:46:28] Completed downloading doc 319\n",
      "[2022-05-13 20:46:31] Completed downloading doc 320\n",
      "[2022-05-13 20:46:36] Completed downloading doc 321\n",
      "[2022-05-13 20:46:41] Completed downloading doc 322\n",
      "[2022-05-13 20:46:45] Completed downloading doc 323\n",
      "[2022-05-13 20:46:48] Completed downloading doc 324\n",
      "[2022-05-13 20:46:50] Completed downloading doc 325\n",
      "[2022-05-13 20:46:53] Completed downloading doc 326\n",
      "[2022-05-13 20:46:57] Completed downloading doc 327\n",
      "[2022-05-13 20:47:00] Completed downloading doc 328\n",
      "[2022-05-13 20:47:03] Completed downloading doc 329\n",
      "[2022-05-13 20:47:06] Completed downloading doc 330\n",
      "[2022-05-13 20:47:08] Completed downloading doc 331\n",
      "[2022-05-13 20:47:15] Completed downloading doc 332\n",
      "[2022-05-13 20:47:18] Completed downloading doc 333\n",
      "[2022-05-13 20:47:21] Completed downloading doc 334\n",
      "[2022-05-13 20:47:25] Completed downloading doc 335\n",
      "[2022-05-13 20:47:33] Completed downloading doc 336\n",
      "[2022-05-13 20:47:42] Completed downloading doc 337\n",
      "[2022-05-13 20:47:46] Completed downloading doc 338\n",
      "[2022-05-13 20:47:49] Completed downloading doc 339\n",
      "[2022-05-13 20:47:52] Completed downloading doc 340\n",
      "[2022-05-13 20:48:00] Completed downloading doc 341\n",
      "[2022-05-13 20:48:07] Completed downloading doc 342\n",
      "[2022-05-13 20:48:10] Completed downloading doc 343\n",
      "[2022-05-13 20:48:17] Completed downloading doc 344\n",
      "[2022-05-13 20:48:20] Completed downloading doc 345\n",
      "[2022-05-13 20:48:23] Completed downloading doc 346\n",
      "[2022-05-13 20:48:27] Completed downloading doc 347\n",
      "[2022-05-13 20:48:33] Completed downloading doc 348\n",
      "[2022-05-13 20:48:37] Completed downloading doc 349\n",
      "[2022-05-13 20:48:41] Completed downloading doc 350\n",
      "[2022-05-13 20:48:46] Completed downloading doc 351\n",
      "[2022-05-13 20:48:49] Completed downloading doc 352\n",
      "[2022-05-13 20:48:52] Completed downloading doc 353\n",
      "[2022-05-13 20:48:57] Completed downloading doc 354\n",
      "[2022-05-13 20:49:01] Completed downloading doc 355\n",
      "[2022-05-13 20:49:07] Completed downloading doc 356\n",
      "[2022-05-13 20:49:11] Completed downloading doc 357\n",
      "[2022-05-13 20:49:13] Completed downloading doc 358\n",
      "[2022-05-13 20:49:15] Completed downloading doc 359\n",
      "[2022-05-13 20:49:20] Completed downloading doc 360\n",
      "[2022-05-13 20:49:24] Completed downloading doc 361\n",
      "[2022-05-13 20:49:28] Completed downloading doc 362\n",
      "[2022-05-13 20:49:32] Completed downloading doc 363\n",
      "[2022-05-13 20:49:37] Completed downloading doc 364\n",
      "[2022-05-13 20:49:43] Completed downloading doc 365\n",
      "[2022-05-13 20:49:52] Completed downloading doc 366\n",
      "[2022-05-13 20:49:56] Completed downloading doc 367\n",
      "[2022-05-13 20:50:01] Completed downloading doc 368\n",
      "[2022-05-13 20:50:09] Completed downloading doc 369\n",
      "[2022-05-13 20:50:13] Completed downloading doc 370\n",
      "[2022-05-13 20:50:17] Completed downloading doc 371\n",
      "[2022-05-13 20:50:22] Completed downloading doc 372\n",
      "[2022-05-13 20:50:25] Completed downloading doc 373\n",
      "[2022-05-13 20:50:29] Completed downloading doc 374\n",
      "[2022-05-13 20:50:32] Completed downloading doc 375\n",
      "[2022-05-13 20:50:36] Completed downloading doc 376\n",
      "[2022-05-13 20:50:39] Completed downloading doc 377\n",
      "[2022-05-13 20:50:42] Completed downloading doc 378\n",
      "[2022-05-13 20:50:45] Completed downloading doc 379\n",
      "[2022-05-13 20:50:49] Completed downloading doc 380\n",
      "[2022-05-13 20:50:52] Completed downloading doc 381\n",
      "[2022-05-13 20:50:55] Completed downloading doc 382\n",
      "[2022-05-13 20:50:59] Completed downloading doc 383\n",
      "[2022-05-13 20:51:04] Completed downloading doc 384\n",
      "[2022-05-13 20:51:07] Completed downloading doc 385\n",
      "[2022-05-13 20:51:15] Completed downloading doc 386\n",
      "[2022-05-13 20:51:20] Completed downloading doc 387\n",
      "[2022-05-13 20:51:24] Completed downloading doc 388\n",
      "[2022-05-13 20:51:27] Completed downloading doc 389\n",
      "[2022-05-13 20:51:33] Completed downloading doc 390\n",
      "[2022-05-13 20:51:39] Completed downloading doc 391\n",
      "[2022-05-13 20:51:41] Completed downloading doc 392\n",
      "[2022-05-13 20:51:47] Completed downloading doc 393\n",
      "[2022-05-13 20:51:51] Completed downloading doc 394\n",
      "[2022-05-13 20:51:54] Completed downloading doc 395\n",
      "[2022-05-13 20:51:58] Completed downloading doc 396\n",
      "[2022-05-13 20:52:01] Completed downloading doc 397\n",
      "[2022-05-13 20:52:09] Completed downloading doc 398\n",
      "[2022-05-13 20:52:11] Completed downloading doc 399\n",
      "[2022-05-13 20:52:16] Completed downloading doc 400\n",
      "[2022-05-13 20:52:18] Completed downloading doc 401\n",
      "[2022-05-13 20:52:21] Completed downloading doc 402\n",
      "[2022-05-13 20:52:25] Completed downloading doc 403\n",
      "[2022-05-13 20:52:28] Completed downloading doc 404\n",
      "[2022-05-13 20:52:34] Completed downloading doc 405\n",
      "[2022-05-13 20:52:37] Completed downloading doc 406\n",
      "[2022-05-13 20:52:41] Completed downloading doc 407\n",
      "[2022-05-13 20:52:46] Completed downloading doc 408\n",
      "[2022-05-13 20:52:48] Completed downloading doc 409\n",
      "[2022-05-13 20:52:56] Completed downloading doc 410\n",
      "[2022-05-13 20:53:05] Completed downloading doc 411\n",
      "[2022-05-13 20:53:14] Completed downloading doc 412\n",
      "[2022-05-13 20:53:19] Completed downloading doc 413\n",
      "[2022-05-13 20:53:23] Completed downloading doc 414\n",
      "[2022-05-13 20:53:26] Completed downloading doc 415\n",
      "[2022-05-13 20:53:33] Completed downloading doc 416\n",
      "[2022-05-13 20:53:37] Completed downloading doc 417\n",
      "[2022-05-13 20:53:44] Completed downloading doc 418\n",
      "[2022-05-13 20:53:47] Completed downloading doc 419\n",
      "[2022-05-13 20:53:50] Completed downloading doc 420\n",
      "[2022-05-13 20:53:54] Completed downloading doc 421\n",
      "[2022-05-13 20:53:59] Completed downloading doc 422\n",
      "[2022-05-13 20:54:05] Completed downloading doc 423\n",
      "[2022-05-13 20:54:08] Completed downloading doc 424\n",
      "[2022-05-13 20:54:11] Completed downloading doc 425\n",
      "[2022-05-13 20:54:14] Completed downloading doc 426\n",
      "[2022-05-13 20:54:22] Completed downloading doc 427\n",
      "[2022-05-13 20:54:24] Completed downloading doc 428\n",
      "[2022-05-13 20:54:29] Completed downloading doc 429\n",
      "[2022-05-13 20:54:34] Completed downloading doc 430\n",
      "[2022-05-13 20:54:37] Completed downloading doc 431\n",
      "[2022-05-13 20:54:40] Completed downloading doc 432\n",
      "[2022-05-13 20:54:45] Completed downloading doc 433\n",
      "[2022-05-13 20:54:49] Completed downloading doc 434\n",
      "[2022-05-13 20:54:53] Completed downloading doc 435\n",
      "[2022-05-13 20:54:58] Completed downloading doc 436\n",
      "[2022-05-13 20:55:02] Completed downloading doc 437\n",
      "[2022-05-13 20:55:05] Completed downloading doc 438\n",
      "[2022-05-13 20:55:09] Completed downloading doc 439\n",
      "[2022-05-13 20:55:10] Completed downloading doc 440\n",
      "[2022-05-13 20:55:12] Completed downloading doc 441\n",
      "[2022-05-13 20:55:15] Completed downloading doc 442\n",
      "[2022-05-13 20:55:20] Completed downloading doc 443\n",
      "[2022-05-13 20:55:24] Completed downloading doc 444\n",
      "[2022-05-13 20:55:27] Completed downloading doc 445\n",
      "[2022-05-13 20:55:34] Completed downloading doc 446\n",
      "[2022-05-13 20:55:40] Completed downloading doc 447\n",
      "[2022-05-13 20:55:44] Completed downloading doc 448\n",
      "[2022-05-13 20:55:46] Completed downloading doc 449\n",
      "[2022-05-13 20:55:50] Completed downloading doc 450\n",
      "[2022-05-13 20:55:53] Completed downloading doc 451\n",
      "[2022-05-13 20:55:56] Completed downloading doc 452\n",
      "[2022-05-13 20:56:00] Completed downloading doc 453\n",
      "[2022-05-13 20:56:07] Completed downloading doc 454\n",
      "[2022-05-13 20:56:13] Completed downloading doc 455\n",
      "[2022-05-13 20:56:17] Completed downloading doc 456\n",
      "[2022-05-13 20:56:20] Completed downloading doc 457\n",
      "[2022-05-13 20:56:23] Completed downloading doc 458\n",
      "[2022-05-13 20:56:27] Completed downloading doc 459\n",
      "[2022-05-13 20:56:32] Completed downloading doc 460\n",
      "[2022-05-13 20:56:36] Completed downloading doc 461\n",
      "[2022-05-13 20:56:39] Completed downloading doc 462\n",
      "[2022-05-13 20:56:44] Completed downloading doc 463\n",
      "[2022-05-13 20:56:50] Completed downloading doc 464\n",
      "[2022-05-13 20:56:53] Completed downloading doc 465\n",
      "[2022-05-13 20:57:05] Completed downloading doc 466\n",
      "[2022-05-13 20:57:08] Completed downloading doc 467\n",
      "[2022-05-13 20:57:14] Completed downloading doc 468\n",
      "[2022-05-13 20:57:18] Completed downloading doc 469\n",
      "[2022-05-13 20:57:23] Completed downloading doc 470\n",
      "[2022-05-13 20:57:26] Completed downloading doc 471\n",
      "[2022-05-13 20:57:30] Completed downloading doc 472\n",
      "[2022-05-13 20:57:47] Completed downloading doc 473\n",
      "[2022-05-13 20:57:50] Completed downloading doc 474\n",
      "[2022-05-13 20:57:54] Completed downloading doc 475\n",
      "[2022-05-13 20:57:58] Completed downloading doc 476\n",
      "[2022-05-13 20:58:01] Completed downloading doc 477\n",
      "[2022-05-13 20:58:05] Completed downloading doc 478\n",
      "[2022-05-13 20:58:08] Completed downloading doc 479\n",
      "[2022-05-13 20:58:11] Completed downloading doc 480\n",
      "[2022-05-13 20:58:15] Completed downloading doc 481\n",
      "[2022-05-13 20:58:26] Completed downloading doc 482\n",
      "[2022-05-13 20:58:30] Completed downloading doc 483\n",
      "[2022-05-13 20:58:34] Completed downloading doc 484\n",
      "[2022-05-13 20:58:42] Completed downloading doc 485\n",
      "[2022-05-13 20:58:46] Completed downloading doc 486\n",
      "[2022-05-13 20:58:49] Completed downloading doc 487\n",
      "[2022-05-13 20:58:53] Completed downloading doc 488\n",
      "[2022-05-13 20:59:03] Completed downloading doc 489\n",
      "[2022-05-13 20:59:07] Completed downloading doc 490\n",
      "[2022-05-13 20:59:12] Completed downloading doc 491\n",
      "[2022-05-13 20:59:18] Completed downloading doc 492\n",
      "[2022-05-13 20:59:21] Completed downloading doc 493\n",
      "[2022-05-13 20:59:25] Completed downloading doc 494\n",
      "[2022-05-13 20:59:33] Completed downloading doc 495\n",
      "[2022-05-13 20:59:36] Completed downloading doc 496\n",
      "[2022-05-13 20:59:41] Completed downloading doc 497\n",
      "[2022-05-13 20:59:44] Completed downloading doc 498\n",
      "[2022-05-13 20:59:47] Completed downloading doc 499\n",
      "[2022-05-13 20:59:54] Completed downloading doc 500\n",
      "[2022-05-13 20:59:59] Completed downloading doc 501\n",
      "[2022-05-13 21:00:05] Completed downloading doc 502\n",
      "[2022-05-13 21:00:14] Completed downloading doc 503\n",
      "[2022-05-13 21:00:19] Completed downloading doc 504\n",
      "[2022-05-13 21:00:24] Completed downloading doc 505\n",
      "[2022-05-13 21:00:27] Completed downloading doc 506\n",
      "[2022-05-13 21:00:30] Completed downloading doc 507\n",
      "[2022-05-13 21:00:34] Completed downloading doc 508\n",
      "[2022-05-13 21:00:37] Completed downloading doc 509\n",
      "[2022-05-13 21:00:41] Completed downloading doc 510\n",
      "[2022-05-13 21:00:46] Completed downloading doc 511\n",
      "[2022-05-13 21:00:50] Completed downloading doc 512\n",
      "[2022-05-13 21:00:54] Completed downloading doc 513\n",
      "[2022-05-13 21:01:01] Completed downloading doc 514\n",
      "[2022-05-13 21:01:11] Completed downloading doc 515\n",
      "[2022-05-13 21:01:18] Completed downloading doc 516\n",
      "[2022-05-13 21:01:23] Completed downloading doc 517\n",
      "[2022-05-13 21:01:28] Completed downloading doc 518\n",
      "[2022-05-13 21:01:33] Completed downloading doc 519\n",
      "[2022-05-13 21:01:36] Completed downloading doc 520\n",
      "[2022-05-13 21:01:42] Completed downloading doc 521\n",
      "[2022-05-13 21:01:49] Completed downloading doc 522\n",
      "[2022-05-13 21:01:54] Completed downloading doc 523\n",
      "[2022-05-13 21:01:59] Completed downloading doc 524\n",
      "[2022-05-13 21:02:04] Completed downloading doc 525\n",
      "[2022-05-13 21:02:15] Completed downloading doc 526\n",
      "[2022-05-13 21:02:19] Completed downloading doc 527\n",
      "[2022-05-13 21:02:21] Completed downloading doc 528\n",
      "[2022-05-13 21:02:25] Completed downloading doc 529\n",
      "[2022-05-13 21:02:28] Completed downloading doc 530\n",
      "[2022-05-13 21:02:32] Completed downloading doc 531\n",
      "[2022-05-13 21:02:36] Completed downloading doc 532\n",
      "[2022-05-13 21:02:42] Completed downloading doc 533\n",
      "[2022-05-13 21:02:45] Completed downloading doc 534\n",
      "[2022-05-13 21:02:50] Completed downloading doc 535\n",
      "[2022-05-13 21:02:55] Completed downloading doc 536\n",
      "[2022-05-13 21:02:59] Completed downloading doc 537\n",
      "[2022-05-13 21:03:03] Completed downloading doc 538\n",
      "[2022-05-13 21:03:09] Completed downloading doc 539\n",
      "[2022-05-13 21:03:12] Completed downloading doc 540\n",
      "[2022-05-13 21:03:17] Completed downloading doc 541\n",
      "[2022-05-13 21:03:21] Completed downloading doc 542\n",
      "[2022-05-13 21:03:22] Completed downloading doc 543\n",
      "[2022-05-13 21:03:27] Completed downloading doc 544\n",
      "[2022-05-13 21:03:32] Completed downloading doc 545\n",
      "[2022-05-13 21:03:36] Completed downloading doc 546\n",
      "[2022-05-13 21:03:40] Completed downloading doc 547\n",
      "[2022-05-13 21:03:42] Completed downloading doc 548\n",
      "[2022-05-13 21:03:46] Completed downloading doc 549\n",
      "[2022-05-13 21:03:50] Completed downloading doc 550\n",
      "[2022-05-13 21:03:54] Completed downloading doc 551\n",
      "[2022-05-13 21:03:59] Completed downloading doc 552\n",
      "[2022-05-13 21:04:03] Completed downloading doc 553\n",
      "[2022-05-13 21:04:08] Completed downloading doc 554\n",
      "[2022-05-13 21:04:12] Completed downloading doc 555\n",
      "[2022-05-13 21:04:18] Completed downloading doc 556\n",
      "[2022-05-13 21:04:21] Completed downloading doc 557\n",
      "[2022-05-13 21:04:25] Completed downloading doc 558\n",
      "[2022-05-13 21:04:29] Completed downloading doc 559\n",
      "[2022-05-13 21:04:35] Completed downloading doc 560\n",
      "[2022-05-13 21:04:39] Completed downloading doc 561\n",
      "[2022-05-13 21:04:42] Completed downloading doc 562\n",
      "[2022-05-13 21:04:46] Completed downloading doc 563\n",
      "[2022-05-13 21:04:51] Completed downloading doc 564\n",
      "[2022-05-13 21:04:55] Completed downloading doc 565\n",
      "[2022-05-13 21:05:00] Completed downloading doc 566\n",
      "[2022-05-13 21:05:05] Completed downloading doc 567\n",
      "[2022-05-13 21:05:10] Completed downloading doc 568\n",
      "[2022-05-13 21:05:14] Completed downloading doc 569\n",
      "[2022-05-13 21:05:17] Completed downloading doc 570\n",
      "[2022-05-13 21:05:22] Completed downloading doc 571\n",
      "[2022-05-13 21:05:25] Completed downloading doc 572\n",
      "[2022-05-13 21:05:29] Completed downloading doc 573\n",
      "[2022-05-13 21:05:32] Completed downloading doc 574\n",
      "[2022-05-13 21:05:36] Completed downloading doc 575\n",
      "[2022-05-13 21:05:40] Completed downloading doc 576\n",
      "[2022-05-13 21:05:43] Completed downloading doc 577\n",
      "[2022-05-13 21:05:46] Completed downloading doc 578\n",
      "[2022-05-13 21:05:50] Completed downloading doc 579\n",
      "[2022-05-13 21:05:54] Completed downloading doc 580\n",
      "[2022-05-13 21:05:58] Completed downloading doc 581\n",
      "[2022-05-13 21:06:03] Completed downloading doc 582\n",
      "[2022-05-13 21:06:08] Completed downloading doc 583\n",
      "[2022-05-13 21:06:17] Completed downloading doc 584\n",
      "[2022-05-13 21:06:20] Completed downloading doc 585\n",
      "[2022-05-13 21:06:25] Completed downloading doc 586\n",
      "[2022-05-13 21:06:30] Completed downloading doc 587\n",
      "[2022-05-13 21:06:35] Completed downloading doc 588\n",
      "[2022-05-13 21:06:42] Completed downloading doc 589\n",
      "[2022-05-13 21:06:46] Completed downloading doc 590\n",
      "[2022-05-13 21:06:50] Completed downloading doc 591\n",
      "[2022-05-13 21:06:54] Completed downloading doc 592\n",
      "[2022-05-13 21:07:00] Completed downloading doc 593\n",
      "[2022-05-13 21:07:04] Completed downloading doc 594\n",
      "[2022-05-13 21:07:07] Completed downloading doc 595\n",
      "[2022-05-13 21:07:12] Completed downloading doc 596\n",
      "[2022-05-13 21:07:16] Completed downloading doc 597\n",
      "[2022-05-13 21:07:20] Completed downloading doc 598\n",
      "[2022-05-13 21:07:23] Completed downloading doc 599\n",
      "[2022-05-13 21:07:26] Completed downloading doc 600\n",
      "[2022-05-13 21:07:30] Completed downloading doc 601\n",
      "[2022-05-13 21:09:26] Vocab size of TFIDF (1-gram): 66420\n",
      "[2022-05-13 21:09:26] Vocab size of TFIDF (2-gram): 2546667\n",
      "[2022-05-13 21:50:02] Vocab size of TFIDF overlapped with Word2Vec: 34496\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Generate a global TFIDF model\n",
    "'''\n",
    "# sample and clean doc\n",
    "master_idx_sampled = master_idx.groupby('cik').last().sort_values('filing_date').reset_index(drop=True)\n",
    "master_idx_sampled = master_idx \\\n",
    "    .sort_values(['cik','filing_date']).reset_index(drop=True) \\\n",
    "    .groupby('cik').last().reset_index() \\\n",
    "\n",
    "def download_doc(i):\n",
    "    url = master_idx_sampled.iloc[i]['url_10k']\n",
    "    txt = requests.get(url, headers={\"user-agent\": f\"chan_tai_man_{int(float(np.random.rand(1)) * 1e7)}@gmail.com\"}).text\n",
    "    txt = soup = BeautifulSoup(txt, 'lxml').get_text('|', strip=True)\n",
    "    txt = clean_doc1(txt)\n",
    "    item_pos = find_item_pos(txt)\n",
    "    item_1 = clean_doc2(txt[item_pos.iloc[0]['item_1_pos_start'] : item_pos.iloc[0]['item_1_pos_end']])\n",
    "    full = clean_doc2(txt[item_pos.iloc[0]['item_1_pos_start'] :])\n",
    "    log(f'Completed downloading doc {i}')\n",
    "    return {'full':full, 'item_1':item_1}\n",
    "docs = {}\n",
    "for i in range(len(master_idx_sampled)):\n",
    "    cik = master_idx_sampled.iloc[i]['cik']\n",
    "    docs[cik] = download_doc(i)\n",
    "doc_list = [docs[cik]['full'] for cik in docs]\n",
    "\n",
    "# build tfidf for 1 and 2 gram\n",
    "global_tfidf_1g = TfidfVectorizer(ngram_range=(1,1), norm='l2', min_df=0.0, max_df=0.7, use_idf=True, binary=False, token_pattern=r\"(?u)\\b[a-z]{3,}\\b\").fit(doc_list)\n",
    "global_tfidf_2g = TfidfVectorizer(ngram_range=(1,2), norm='l2', min_df=0.0, max_df=0.7, use_idf=True, binary=False, token_pattern=r\"(?u)\\b[a-z]{3,}\\b\").fit(doc_list)\n",
    "log(f'Vocab size of TFIDF (1-gram): {len(global_tfidf_1g.vocabulary_)}')\n",
    "log(f'Vocab size of TFIDF (2-gram): {len(global_tfidf_2g.vocabulary_)}')\n",
    "\n",
    "# release memory\n",
    "save_pkl(docs, 'docs')\n",
    "del doc_list, docs\n",
    "gc.collect()\n",
    "\n",
    "# download word2vec\n",
    "import gensim.downloader as api\n",
    "wv = load_pkl('../input/word2vecgooglenews300/wv')\n",
    "# for i in range(20):\n",
    "#     try:\n",
    "#         wv = api.load('word2vec-google-news-300')\n",
    "#         break\n",
    "#     except:\n",
    "#         continue\n",
    "        \n",
    "# get the column index for vocab overlapping with Word2Vec\n",
    "wv_vocab_list = list(wv.key_to_index)\n",
    "tfidf_vocab = global_tfidf_1g.vocabulary_\n",
    "tfidf_vocab_swap = {v: k for k, v in tfidf_vocab.items()}\n",
    "tfidf_1g_wv_idx = sorted([global_tfidf_1g.vocabulary_[x] for x in list(global_tfidf_1g.vocabulary_) if x in wv_vocab_list])\n",
    "tfidf_1g_wv_word = [tfidf_vocab_swap[x] for x in tfidf_1g_wv_idx]\n",
    "log(f'Vocab size of TFIDF overlapped with Word2Vec: {len(tfidf_1g_wv_idx)}')\n",
    "\n",
    "# extract smaller word2vec dict\n",
    "wv_subset = {w : wv[w] for w in tfidf_1g_wv_word}\n",
    "del wv\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "825f4947",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-13T13:50:03.339117Z",
     "iopub.status.busy": "2022-05-13T13:50:03.338358Z",
     "iopub.status.idle": "2022-05-13T13:50:19.626303Z",
     "shell.execute_reply": "2022-05-13T13:50:19.625802Z"
    },
    "papermill": {
     "duration": 16.524141,
     "end_time": "2022-05-13T13:50:19.626470",
     "exception": false,
     "start_time": "2022-05-13T13:50:03.102329",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-05-13 21:50:19] Shape of 8-K feats: (1261848, 4)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Count of 8-K filings\n",
    "'''\n",
    "# get a vector of all calendar dates\n",
    "dates = pd.to_datetime(pd.Series(['2008-01-01'] * 365*11)) \\\n",
    "    .to_frame() \\\n",
    "    .rename(columns={0:'filing_date'})\n",
    "dates['filing_date'] = dates['filing_date'] + pd.Series([np.timedelta64(i, 'D') for i in range(len(dates))])\n",
    "\n",
    "# count the rolling 1-year number of 8-K filings\n",
    "feats_8k = []\n",
    "for cik in master_idx_8k.cik.unique():\n",
    "    df = pd.merge(dates, master_idx_8k.loc[lambda x: x.cik==cik], how='left', on='filing_date') \\\n",
    "        .assign(filed=lambda x: x.cik.notnull().astype(int)) \\\n",
    "        .assign(cik=cik) \\\n",
    "        .loc[:,['cik','filing_date','filed']] \\\n",
    "        .assign(feat_cnt_8k = lambda x: x.rolling(365).filed.sum()) \\\n",
    "        .dropna()\n",
    "    feats_8k.append(df)\n",
    "feats_8k = pd.concat(feats_8k).reset_index(drop=True)\n",
    "\n",
    "# calculate Year-on-Year change\n",
    "feats_8k = feats_8k \\\n",
    "    .merge(cik_map, how='inner', on='cik') \\\n",
    "    .rename(columns={'filing_date':'date'}) \\\n",
    "    .loc[:,['stock','date','feat_cnt_8k']] \\\n",
    "    .sort_values(['stock','date']) \\\n",
    "    .assign(feat_cnt_8k_prev = lambda x: x.groupby('stock').feat_cnt_8k.shift(365)) \\\n",
    "    .assign(feat_cnt_8k_diff = lambda x: x.feat_cnt_8k - x.feat_cnt_8k_prev) \\\n",
    "    .assign(feat_cnt_8k = lambda x: x.feat_cnt_8k * -1,\n",
    "            feat_cnt_8k_diff = lambda x: x.feat_cnt_8k_diff * -1) \\\n",
    "    .loc[lambda x: x.date.isin(ret.index), ['stock','date','feat_cnt_8k','feat_cnt_8k_diff']] \\\n",
    "    .dropna()\n",
    "log(f'Shape of 8-K feats: {feats_8k.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1a78c745",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-13T13:50:20.081487Z",
     "iopub.status.busy": "2022-05-13T13:50:20.080510Z",
     "iopub.status.idle": "2022-05-13T13:50:22.575262Z",
     "shell.execute_reply": "2022-05-13T13:50:22.575872Z"
    },
    "papermill": {
     "duration": 2.72438,
     "end_time": "2022-05-13T13:50:22.576076",
     "exception": false,
     "start_time": "2022-05-13T13:50:19.851696",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# export prepared files\n",
    "save_pkl(cik_map, 'cik_map')\n",
    "save_pkl(master_idx, 'master_idx')\n",
    "save_pkl(master_idx_10q, 'master_idx_10q')\n",
    "save_pkl(master_idx_8k, 'master_idx_8k')\n",
    "save_pkl(feats_8k, 'feats_8k')\n",
    "save_pkl(master_idx_sampled, 'master_idx_sampled')\n",
    "save_pkl(global_tfidf_1g, 'global_tfidf_1g')\n",
    "save_pkl(global_tfidf_2g, 'global_tfidf_2g')\n",
    "save_pkl(tfidf_1g_wv_idx, 'tfidf_1g_wv_idx')\n",
    "save_pkl(tfidf_1g_wv_word, 'tfidf_1g_wv_word')\n",
    "save_pkl(wv_subset, 'wv_subset')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 7695.164005,
   "end_time": "2022-05-13T13:50:24.740707",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-05-13T11:42:09.576702",
   "version": "2.3.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "bbe1358039c209e6a130b3211c7f40aaf4f69f3362b7eae5fe32d2d498114034"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
